{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Conv2D, SeparableConv2D, concatenate, MaxPooling2D, GlobalAveragePooling2D \n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X_train,X_test):\n",
    "        #this function normalize inputs for zero mean and unit variance\n",
    "        # it is used when training a model.\n",
    "        # Input: training set and test set\n",
    "        # Output: normalized training set and test set according to the trianing set statistics.\n",
    "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7)\n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = normalize(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = np.unique(y_test)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'airplane',\n",
       " 1: 'automobile',\n",
       " 2: 'bird',\n",
       " 3: 'cat',\n",
       " 4: 'deer',\n",
       " 5: 'dog',\n",
       " 6: 'frog',\n",
       " 7: 'horse',\n",
       " 8: 'ship',\n",
       " 9: 'truck'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "classes = dict(zip(keys,values))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGfhJREFUeJztnX2MXOV1xp/jnfWO8Tge4zFe10u9iZfixFZY0hW4NUrcQBOXohrUJgG1iEokDlWQgpRURbRpiJqqSduEoraCmI/ECTRA+QqKUAlYMYhWdrImi7Fjk6zROKy9a3sWz7JDPMZjn/4x1+3i3HNmPDtzZ533+UnW7t4z773n3rmPZ+Z95pxXVBWEkPCY1e4ECCHtgeInJFAofkICheInJFAofkICheInJFAofkICheInJFAofkICJTWdwSKyDsCdADoA3KuqX/EePz+X1cW9S2Jj3vcMhw8cig94306cmzZDnbPt037/OXOdTM6cAyUjdwDlkyedkfa5dUDM2LJ3LasnrdPwboNfmpG9b75hxoonjH0eP2EfanzcSeO4HfOwns60fQ1nddn3QMfsTjN2/FjZzsO7VyvGfXCsYo+ZtO8dVbVPbgrS6Nd7RaQDwM8A/D6AEQA/BnCdqv7UGvNbA+/Vfx38ZmzMOU1c9YV/jw9UnIu9eoUZ6lmaM2OvD1zqZHLmfOFFI3cAw2+VnJH2Fcl02GK954p76knrNOzrAQyZkWuee9CMPTlh7HOkaB/qgU12bHDUjnmsNravmGMOmdf322Zsfk+3GRvJD9t5lJ07vGDcB3nnP8PnJs1QveKfztv+SwAMq+prqvo2gIcArJ/G/gghCTId8S8F8PqUv0eibYSQs4DpiD/urcWvfIYQkQ0iMigigxOHnbd8hJBEmY74RwCcP+XvHgAHTn+Qqm5U1QFVHZi/KDuNwxFCmsl0xP9jABeIyLtFZDaAawE81Zy0CCGtpmGrT1UrInIzgGdQtfruV9VdNUbBmsVOufP9RqzizJZX7P31OseqYMKMpQzfqAB7Vnbn/j32sSrO5U/ZOZac/IvYHrs9i5X2seC5DnYsPT5y5sNKjkPTCqxLVT5qDpks2Oc1WdhrH2ukYMdSznNdMnLJ20OawbR8flV9GsDTTcqFEJIg/IYfIYFC8RMSKBQ/IYFC8RMSKBQ/IYEyrdn+M8e2+jyjz7VJLMq2pdTj2GiAPa6MjtjtJdjfXBzL7zdjqbRTvOOcci7TZcZKiLepslho7xCORYW3zEjKLVYxrknJGePYsw1jPTXeKXseW26BGepctcaMreztNWNDQ4NGYKedRxPgKz8hgULxExIoFD8hgULxExIoFD8hgZLwbL+gwzhk2ZllN9P02ng5M8dZb5xzSayugGMFu9VVoWC3n0rbbQbdSfExJ1Y2fZOMOaYCp1jFu0UcRwVlo7Kn4px0Cyb7YXXW8jqX2ZfKPedKyi6CGhraYu+zaDgqTt1UM+ArPyGBQvETEigUPyGBQvETEigUPyGBQvETEigJW302lYqzjJNlAXmWnXNmawf6GhkGGP39Jop2YU/Z6VbutfDLOI2OvRyfefiB2O0jS+3VZNZe9llnjy+bEa+XIIpjRsBe8QYVt9qmuXi2otfSMGX3/tMRxzL1bNGt9j5bCV/5CQkUip+QQKH4CQkUip+QQKH4CQkUip+QQJmW1ScieQCTAE4AqKjqgPd4heKY1cPPrdAzYlblGIB5qbwZ+4RbLvVNJ/bfsVsrBbvXWo+zt7RTPZZ32rflv2/HbsaLRsTaDtxy031m7I67fmHGvvmJ58zYk18y+tIVHDuv/CuLPLcHp/DQjY0dsWNeOz7H/WwlzfD5f09VEzRoCSHNgG/7CQmU6YpfAfxARLaLyIZmJEQISYbpvu1fo6oHROQ8AM+KyB5VfWHqA6L/FDYAwKLfXDzNwxFCmsW0XvlV9UD08xCAJwBcEvOYjao6oKoD8xfNn87hCCFNpGHxi8hcEZl36ncAH4E/p0kImUFM523/YgBPiMip/fyHqv6XN0BVUWlkSSZrTMm2ByfHbBvwLx/cYsb+6U8NiwoA8Hrs1ov6rjdHrLULCFG0+35ii2PnNZt/udu2qPr65pmxlNcHdY+xPee4wcec/TUbz17zbtF8g+NcE7w9NCx+VX0NwEVNzIUQkiC0+ggJFIqfkECh+AkJFIqfkECh+AkJlEQbeM6CYq7h56TcTCxPyasEtBdjyzjrtG3Zalf8rV39rtjtPblJc8w//JV9rIt/w47NFG7+vB3r7HUGmlaa0x3TaXbadLY5sY86sd4Gj+fZgEme9xT4yk9IoFD8hAQKxU9IoFD8hAQKxU9IoCQ621/45TF8YzBvJOJMh5aNWNpJv2LPKjs1P8gtt2MPPhY/q+/VKmWcnm9Do3bsbOB43glasYF99pgGar4axrvzezqdoPOEWvcp4C/Xhfb0LuQrPyGBQvETEigUPyGBQvETEigUPyGBQvETEiiJWn1HRo/g0b9/yMjEsUJSxrpWacd2cZb/yjtWn1crtHLlmY/Z+7JzrBApHrVjx5NLw3vOXO+2UcV4lWupJE/8/+ErPyGBQvETEigUPyGBQvETEigUPyGBQvETEig1jQsRuR/AVQAOqeqqaNu5AB5GtaNZHsDHVdVe8+kUJ0/a1U05w87z8JrxOdVXI47V5ziEZve5bsdxXOxZSiEy3O4EIjy715NFo1afdx+4ubSOel75vwVg3WnbbgWwWVUvALA5+psQchZRU/yq+gKAN07bvB7Apuj3TQCubnJehJAW0+hn/sWqOgoA0c/zmpcSISQJWv71XhHZAGADACA9u9WHI4TUSaOv/AdFZAkARD8PWQ9U1Y2qOqCqA+hMtJSAEOLQqPifAnBD9PsNAL7XnHQIIUlRj9X3XQBrAeREZATAFwF8BcAjInIjgF8A+FhdRzt+FBjZFR8reI0RjfWMsk76eTu203EIy312LDvfGmSP6V9jx852epyYvejZWYBX1deoDZjxgu2p6qspflW9zghd3uRcCCEJwm/4ERIoFD8hgULxExIoFD8hgULxExIoyX7rJqVAt9XA0WnsWDC273UskrITi19yDwDwR5vt2Pyl8dtzjkuZ6PpzLWBOuxNoFV5BaKPPWSsq/loIX/kJCRSKn5BAofgJCRSKn5BAofgJCRSKn5BASdTqO3fBPFx59aWxsa3De8xxw3uMGjG3isqJOQ0TPdelaIzbZxQdAsDxcWeHZwGOAXt2V+6NOrFs1o55FX9l58ZKOX7wDG7gSQj5NYTiJyRQKH5CAoXiJyRQKH5CAiXR2f7ZkkJPOr6i4qq+AXNcPmN0i0vZ6aednmkrstvNWH/WrvqxDld0ZvtLY3bspr+xY3+21o5t3mLHvEnsmYA4MU0sixrkrGaNAIrH7FjaGefcqyjla6bUCvjKT0igUPyEBArFT0igUPyEBArFT0igUPyEBEo9y3XdD+AqAIdUdVW07XYAnwJwOHrYbar6dK19ndOVRv/yFbGx0gmnpMYotEg79kk2bcf607Y3t9xxa9Kp+KKOysKD5phi9rAZu+Uq+1i57k4ztnnLmS/v9JO7rzZjF9/05Bnvr1E+Pc+OPXahHTs82OREvOaEj+btmFeE0+AqX2hT8Vc9r/zfArAuZvsdqtof/aspfELIzKKm+FX1BQBvJJALISRBpvOZ/2YR2SEi94vIgqZlRAhJhEbFfxeA5QD6Uf1G6desB4rIBhEZFJHBNyd+2eDhCCHNpiHxq+pBVT2hqicB3APgEuexG1V1QFUH3jX/nEbzJIQ0mYbELyJLpvx5DYCdzUmHEJIU9Vh93wWwFkBOREYAfBHAWhHpR7UQKw/g0/Uc7NCbE/i3Z74fG0tn7B5nxZJhAzrZd+fsYCpn+zUVZ6dzzcaAc80xb3neUNbukJfO2E0I1191xIx9wyhYvPMBp7wwQSpr7NiO79xrxpYs+mRzE/GaE+5yYo0uzebFnKrQVlJT/Kp6Xczm+1qQCyEkQfgNP0ICheInJFAofkICheInJFAofkICRVSTa5so80Xxu0YLR89CKRk5emtrObHOXju23FmpqWzs03HlUHJsnPxWOwa7UBArltmx1ZfFVwOmCvaYe5888yrBswares+xHJGzKypdW67kXEdvXJO/JaOqXp/U/4Ov/IQECsVPSKBQ/IQECsVPSKBQ/IQECsVPSKAka/WdIwqrSWOvY6+MGRaKV5bUY+9PirYlc2H8UoIA7MKsilOxNeEU9R3ZY8ew3w7dckOvGVtlWJXd3X3mmJJzApm0sU4igJRTsVgyFikcLtpj7n3I9ryG82YoWTwTrdsOrbp0kRnLpOOrQrc+nLd36MiWVh8hxIXiJyRQKH5CAoXiJyRQKH5CAqVmG6+m0gHAKpwpOalYBRM9zqSmU2SR8oqI0s4+K/E59vbaFkF+ZNSM2Z34gBWX227FH6+7zIxljAt80Jh9B4CMc+0rFaciyKnGSnfHT31feukqc8yH1ttLiu3fazsB+WE7x61D8U0Nn99mN/E77J2yVwNlP9Uo5u1l2y67YmV8YOU+c0zJKHYbPmDncDp85SckUCh+QgKF4ickUCh+QgKF4ickUCh+QgKlZmGPiJwP4Nuoli2cBLBRVe8UkXMBPAygF9Uluz6uqp57BVkwS7HWsIdGnPWTrH58487BDPcEANCYe2Ubo95KWHZdjDvuy59ca8YuX2HbZVaNTirlFO/YaaBUdJI0rE8AmDAS6Vhm26Iruh3L9NVhMzZesBs2poxl4Cpp+4kuO+dVKjlVXClnnLNeV6kQH8vvtO3Nwlj8zb/lfwo4MvF20wp7KgA+p6rvBbAawGdE5H0AbgWwWVUvALA5+psQcpZQU/yqOqqqL0W/TwLYDWApgPUANkUP2wTA/oYGIWTGcUaf+UWkF8DFALYBWKyqo0D1PwgA5zU7OUJI66j7670ikgHwGIBbVPVNkbo+VkBENgDYAACYU98YQkjrqeuVX0Q6URX+g6r6eLT5oIgsieJLAByKG6uqG1V1QFUH0NWMlAkhzaCm+KX6En8fgN2q+vUpoacA3BD9fgOA7zU/PUJIq6jnbf8aANcDeEVEhqJttwH4CoBHRORGAL8A8LGae3pbTUtvzooF5rCjRcNBXDHPOZizXle5seWpOg1PLL3K/jgz6TRb6++21pICeh3baKLoeJWV+PMuFOz1olKO7ZVOe0agk4Zxa43ts/N4frNtbVWc3n+lsm2jpXPx55ZxbMVsxmnk6JBy8ig467ZVDKsyVbLv4e5MfN+/zo43zDG/sv9aD1DVF2G3Lby87iMRQmYU/IYfIYFC8RMSKBQ/IYFC8RMSKBQ/IYGSbANPsY94tGAXBC7qWRK7/XDRtk+Qd+w876wdZyuXjbfmis5yV52OrXjTuo+asT9c3m/GxuDYRuV4S2xuxj5pz84bL9gWW5djEe4zxqWzVgdXoLvHLoEcK9tVfdmsfW5ly3J0zqvkNDQtOZZd2usM61i3ZeP+GXPuq6xxrJPuemLvhK/8hAQKxU9IoFD8hAQKxU9IoFD8hAQKxU9IoCRr9VVgNs9cdZndlLLbqMDaPvyqfaiMbV+VCva6abmcXV1YMKzFlGPxXJizK8Q+dJF9zpmsnX93yT5esWxYUU6OrkXldDS1LCoAWHlh/HmXHctrv3M35lLxa/8BQCZrX+Mxo2Iu71RGVpzzyjlWZdqRU8a5Hyu5+HFpp0rQKlrtmNVhjzkNvvITEigUPyGBQvETEigUPyGBQvETEijJzvZ3AeiLLzzYX7ILLXY+F9/bbY5R8AP4M7bpjD2jf3jYWXHMmPg+XrSXGrvi2jVmLJexZ46NFZwAABmnoCaXji+OyRkzygBQLHr9Du3luiZK9oz5/rH4cSXnefHcg96sPdvflbKvYyYbf249jsFRdPoFptP2sbzn06Nk9FfscZwFo1UjOjs4208IqQHFT0igUPyEBArFT0igUPyEBArFT0ig1LT6ROR8AN8G0A3gJICNqnqniNwO4FMATlXJ3KaqT3v76krNwvm5+GWGxov77YFGrcrRwqg9xnGvjh+0Y7BdHsBwXuZcZA/5u09cbca84h2UXK/Pjlk4NloqZccqsGPd3U7PPeM67tuXN8eknaKf+U6fwexCu7AnZ/izhYO2jXbCcewq3o3lqKlStsdZT3XaKQaC0ZNxVqp+q68en78C4HOq+pKIzAOwXUSejWJ3qOo/1300QsiMoZ61+kYBjEa/T4rIbgBLW50YIaS1nNFnfhHpBXAxgG3RpptFZIeI3C8i9tfmCCEzjrrFLyIZAI8BuEVV3wRwF4DlAPpRfWfwNWPcBhEZFJHBE8fs5aoJIclSl/hFpBNV4T+oqo8DgKoeVNUTqnoSwD0ALokbq6obVXVAVQc6uupfUIAQ0lpqil9EBMB9AHar6tenbJ9aVXMNgPjqG0LIjKSe2f41AK4H8IqIDEXbbgNwnYj0A1AAeQCfrrWjvsXn4dHP/nlsrOQ4KOMT8b7RcH7EHJPrte2fdLzbCAAY2583YyNGTzWnHRwe3m7/n1hxKhl7HDtv4ULbiyoZllLaqQTMphwbLW1X03lPWroSf2tdutzuW1gctysIK44Fm/GKEo2mkb3L7DGlom1vlo7ZkvGWNvOW+cqm4/dZcqotx4xKwBPHHYv4NOqZ7X8RiF0AzPX0CSEzG37Dj5BAofgJCRSKn5BAofgJCRSKn5BASbSBp6ggVYm3nCy7AwC6u+OtrWUX9jpHs/eXc6qlcmvtKjyr5G/W71xrjtDtW+zdfbTTDPX2LDZjE2MTZqxUmozdvqzbbnaadY61f8S2qLqc4sJxI8fFObssZKIwbsZSjte3tNvOP2XYs5mUbQVnnUacjmOKkbx9rXr7nCajRuVk2qlkLBhVmsdQ/7do+cpPSKBQ/IQECsVPSKBQ/IQECsVPSKBQ/IQESqJWX2dnyrTt4DSKTFnWi1P1VPTKBJ1GkeUx26750mNPxm7Xl+1DwenBuMip3MsP2RWLzqVCZ++c2O37naqy8bx9rY54axdmnf4Mhic2udWp/N5jh2JLyyJGM/H2JgDAcgGH887BwoCv/IQECsVPSKBQ/IQECsVPSKBQ/IQECsVPSKAkavWdOHESJcuCc+y3SiW+CaPXlLIv51RmmRFgbMRuIvnAi8/EB9Y4O3RsucNDjkXlLF1o2lcAMqmFRhp2JeCRgmPneZSdCrJdR+O3O40z0efEnPXzPObk4isnj+aO24N2OTt0rr04vU7VafKKV61BzpgmwFd+QgKF4ickUCh+QgKF4ickUCh+QgKl5my/iKQBvACgK3r8o6r6RRF5N4CHAJwL4CUA16vq240mUizahSeoGA5B1pkCNnq3AYCzGhM2PfOcGRvZaczOe1fRm7V3lg3DRU7MNiRw5HmjIKjL2d+q+GKg6rGMWXvAP+8LrUocZ1DKmYH3LBprthzA0VeNfXr7c4qxPPdGnefFMVtaPqtvUc8r/zEAH1bVi1BdjnudiKwG8FUAd6jqBQCOALixdWkSQppNTfFrlVOtUzujfwrgwwAejbZvAuC1vSWEzDDq+swvIh3RCr2HADwLYC+AoqqeehM0AsDuyUwImXHUJX5VPaGq/QB6AFwC4L1xD4sbKyIbRGRQRAbHj7zVeKaEkKZyRrP9qloEsAXAagBZETk1e9MD4IAxZqOqDqjqwMIF3gwXISRJaopfRBaJSDb6fQ6AKwDsBvBDAH8SPewGAN9rVZKEkOZTT2HPEgCbRKQD1f8sHlHV74vITwE8JCJfBvATAPfV2tHk5FvY/PzW2FihaFc+5LLxFRO5rL3k0tzGXEA8l7cbyS3pXxC7fVm3Pd1RLtv98YZ2DtuJOC0IYa9qVfVd4nDcPKQdO69RS2zM8K8qjp3nfSo86MQarEsy8WavvD6DzcbpW9gMe7Cm+FV1B4CLY7a/hurnf0LIWQi/4UdIoFD8hAQKxU9IoFD8hAQKxU9IoIhqciVFInIYwL7ozxwAr7NZUjCPd8I83snZlscyVV1Uzw4TFf87DiwyqKoDbTk482AezINv+wkJFYqfkEBpp/g3tvHYU2Ee74R5vJNf2zza9pmfENJe+LafkEBpi/hFZJ2IvCoiwyJyaztyiPLIi8grIjIkIoMJHvd+ETkkIjunbDtXRJ4VkZ9HP+NLCFufx+0isj+6JkMicmUCeZwvIj8Ukd0isktEPhttT/SaOHkkek1EJC0iPxKRl6M8vhRtf7eIbIuux8MiMntaB1LVRP8B6EC1Ddh7AMwG8DKA9yWdR5RLHkCuDcf9IIAPANg5Zds/Arg1+v1WAF9tUx63A/h8wtdjCYAPRL/PA/AzAO9L+po4eSR6TVAt5s1Ev3cC2IZqA51HAFwbbb8bwF9M5zjteOW/BMCwqr6m1VbfDwFY34Y82oaqvgDgjdM2r0e1ESqQUENUI4/EUdVRVX0p+n0S1WYxS5HwNXHySBSt0vKmue0Q/1IAr0/5u53NPxXAD0Rku4hsaFMOp1isqqNA9SYEcF4bc7lZRHZEHwta/vFjKiLSi2r/iG1o4zU5LQ8g4WuSRNPcdog/rj9JuyyHNar6AQB/AOAzIvLBNuUxk7gLwHJU12gYBfC1pA4sIhkAjwG4RVXfTOq4deSR+DXRaTTNrZd2iH8EwPlT/jabf7YaVT0Q/TwE4Am0tzPRQRFZAgDRz0PtSEJVD0Y33kkA9yChayIinagK7kFVfTzanPg1icujXdckOvYZN82tl3aI/8cALohmLmcDuBbAU0knISJzRWTeqd8BfATATn9US3kK1UaoQBsbop4SW8Q1SOCaiIig2gNyt6p+fUoo0Wti5ZH0NUmsaW5SM5inzWZeiepM6l4Af92mHN6DqtPwMoBdSeYB4Luovn08juo7oRsBLASwGcDPo5/ntimP7wB4BcAOVMW3JIE8LkP1LewOAEPRvyuTviZOHoleEwDvR7Up7g5U/6P52yn37I8ADAP4TwBd0zkOv+FHSKDwG36EBArFT0igUPyEBArFT0igUPyEBArFT0igUPyEBArFT0ig/C/yo0YABLA0FAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = np.random.choice(50000)\n",
    "plt.imshow(x_train[nb], interpolation='nearest')\n",
    "plt.show()\n",
    "classes[y_train[nb][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddb_b(X_input, growth, repeat=6, **kwargs):\n",
    "    X = X_input\n",
    "    for i in range(repeat):\n",
    "        X = Conv2D(growth, (1,1), strides = 1, **kwargs)(X_input)\n",
    "        X = BatchNormalization()(X)\n",
    "        X = Activation('relu')(X)\n",
    "        X = SeparableConv2D(growth, (3,3), strides=1, **kwargs)(X)\n",
    "        X = BatchNormalization()(X)\n",
    "        X = Activation('relu')(X)\n",
    "        \n",
    "        X_input = concatenate([X_input, X], axis = -1)\n",
    "    return X_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tinyDSOD_backbone(input_shape, l2):\n",
    "    regul = regularizers.l2(l2)\n",
    "    kwargs = {'padding':'same', 'kernel_regularizer':regul,'kernel_initializer':'glorot_uniform'}\n",
    "\n",
    "    ### STEM ###\n",
    "    # Convolution 1\n",
    "    inp = Input(input_shape)\n",
    "    X = Conv2D(64, (3,3), strides=2, **kwargs)(inp)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Convolution 2\n",
    "    X = Conv2D(64, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Depth-wise seperable convolution 1\n",
    "    X = SeparableConv2D(64, (3,3), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Convolution 3\n",
    "    X = Conv2D(128, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Depth-wise seperable convolution 2\n",
    "    X = SeparableConv2D(128, (3,3), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Pooling\n",
    "    X = MaxPooling2D((2,2), strides=2, padding='same')(X)\n",
    "    \n",
    "    ### Extractor ###\n",
    "    # Dense stage 0\n",
    "    X = ddb_b(X, 32, repeat=4, **kwargs)\n",
    "    \n",
    "    # Transition layer 0\n",
    "    X = Conv2D(128, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2,2), strides=2, padding='same')(X)\n",
    "    \n",
    "    # Dense stage 1\n",
    "    X = ddb_b(X, 48, repeat = 6, **kwargs)\n",
    "    \n",
    "    # Transition layer 1\n",
    "    X = Conv2D(128, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2,2), strides=2, padding='same')(X)\n",
    "    \n",
    "    # Dense stage 2\n",
    "    X = ddb_b(X, 64, repeat=6, **kwargs)\n",
    "    \n",
    "    # Transition layer 2\n",
    "    X = Conv2D(256, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Dense stage 3\n",
    "    X = ddb_b(X, 80, repeat=6, **kwargs)\n",
    "    \n",
    "    # Transition layer 3\n",
    "    X = Conv2D(64, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    \n",
    "    ### Classification layers ###\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    X = Dense(10, activation='softmax', kernel_regularizer=regul, kernel_initializer='glorot_uniform')(X)\n",
    "    \n",
    "    ### Create Model ### \n",
    "    \n",
    "    model = Model(inputs=inp, outputs=X, name='TinyDSOD_bb')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = tinyDSOD_backbone((32,32,3), l2=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 64)   1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 64)   4160        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 16, 16, 64)   4736        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 128)  8320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 16, 16, 128)  17664       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 128)  512         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 128)    0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 32)     4128        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 32)     128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 32)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 8, 8, 32)     1344        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 32)     128         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 32)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 160)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 8, 32)     5152        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 32)     128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 32)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 8, 8, 32)     1344        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 32)     128         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 32)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 192)    0           concatenate_1[0][0]              \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 32)     6176        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 32)     128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 32)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 8, 8, 32)     1344        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 32)     128         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 32)     0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 224)    0           concatenate_2[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 8, 32)     7200        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 32)     128         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 32)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 8, 8, 32)     1344        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 32)     128         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 32)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8, 8, 256)    0           concatenate_3[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 128)    32896       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 128)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 128)    0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 4, 4, 48)     6192        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 4, 4, 48)     192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 4, 4, 48)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 4, 4, 48)     2784        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 4, 4, 48)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 4, 4, 176)    0           max_pooling2d_2[0][0]            \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 4, 4, 48)     8496        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4, 4, 48)     192         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 4, 4, 48)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 4, 4, 48)     2784        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 4, 4, 48)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 4, 4, 224)    0           concatenate_5[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 4, 4, 48)     10800       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 4, 4, 48)     192         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 4, 4, 48)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 4, 4, 48)     2784        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 4, 4, 48)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 4, 4, 272)    0           concatenate_6[0][0]              \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 4, 4, 48)     13104       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 4, 4, 48)     192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 4, 4, 48)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 4, 4, 48)     2784        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 48)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 4, 4, 320)    0           concatenate_7[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 4, 4, 48)     15408       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 4, 4, 48)     192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 48)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 4, 4, 48)     2784        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 48)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 4, 4, 368)    0           concatenate_8[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 4, 4, 48)     17712       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 48)     192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 48)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 4, 4, 48)     2784        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 48)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 4, 4, 416)    0           concatenate_9[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 4, 4, 128)    53376       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 128)    512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 128)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 2, 2, 128)    0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 2, 2, 64)     8256        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 2, 2, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 2, 2, 64)     0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 2, 2, 64)     4736        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 2, 2, 64)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 2, 2, 192)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 2, 2, 64)     12352       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 2, 2, 64)     256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 2, 2, 64)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 2, 2, 64)     4736        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 2, 2, 64)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 2, 2, 256)    0           concatenate_11[0][0]             \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 2, 2, 64)     16448       concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 2, 2, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 2, 2, 64)     0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 2, 2, 64)     4736        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 2, 2, 64)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 2, 2, 320)    0           concatenate_12[0][0]             \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 2, 2, 64)     20544       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 2, 2, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 2, 2, 64)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 2, 2, 64)     4736        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 2, 2, 64)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 2, 2, 384)    0           concatenate_13[0][0]             \n",
      "                                                                 activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 2, 2, 64)     24640       concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 2, 2, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 2, 2, 64)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 2, 2, 64)     4736        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 2, 2, 64)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 2, 2, 448)    0           concatenate_14[0][0]             \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 2, 2, 64)     28736       concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 2, 2, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 2, 2, 64)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 2, 2, 64)     4736        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 2, 2, 64)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 2, 2, 512)    0           concatenate_15[0][0]             \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 2, 2, 256)    131328      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 2, 2, 256)    1024        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 2, 2, 256)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 2, 2, 80)     20560       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 2, 2, 80)     320         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 2, 2, 80)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 2, 2, 80)     7200        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 2, 2, 80)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 2, 2, 336)    0           activation_40[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 2, 2, 80)     26960       concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 2, 2, 80)     320         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 2, 2, 80)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 2, 2, 80)     7200        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 2, 2, 80)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 2, 2, 416)    0           concatenate_17[0][0]             \n",
      "                                                                 activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 2, 2, 80)     33360       concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 2, 2, 80)     320         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 2, 2, 80)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 2, 2, 80)     7200        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 2, 2, 80)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 2, 2, 496)    0           concatenate_18[0][0]             \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 2, 2, 80)     39760       concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 2, 2, 80)     320         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 2, 2, 80)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 2, 2, 80)     7200        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 2, 2, 80)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 2, 2, 576)    0           concatenate_19[0][0]             \n",
      "                                                                 activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 2, 2, 80)     46160       concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 2, 2, 80)     320         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 2, 2, 80)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 2, 2, 80)     7200        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 2, 2, 80)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 2, 2, 656)    0           concatenate_20[0][0]             \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 2, 2, 80)     52560       concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 2, 2, 80)     320         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 2, 2, 80)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 2, 2, 80)     7200        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 2, 2, 80)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 2, 2, 736)    0           concatenate_21[0][0]             \n",
      "                                                                 activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 2, 2, 64)     47168       concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 2, 2, 64)     256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 2, 2, 64)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 834,826\n",
      "Trainable params: 827,658\n",
      "Non-trainable params: 7,168\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL.h5\n",
      "Epoch 1/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 3.0354 - acc: 0.4052 - val_loss: 2.7201 - val_acc: 0.4079\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 2.0310 - acc: 0.5445 - val_loss: 1.9313 - val_acc: 0.5228\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.5999 - acc: 0.5997 - val_loss: 2.0234 - val_acc: 0.4204\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.3964 - acc: 0.6345 - val_loss: 1.5447 - val_acc: 0.5992\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.2818 - acc: 0.6615 - val_loss: 1.4384 - val_acc: 0.6105\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.2171 - acc: 0.6820 - val_loss: 1.7899 - val_acc: 0.5214\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.1762 - acc: 0.6995 - val_loss: 1.4530 - val_acc: 0.6106\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.1389 - acc: 0.7126 - val_loss: 1.5924 - val_acc: 0.5977\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.1129 - acc: 0.7246 - val_loss: 1.1407 - val_acc: 0.7180\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.0878 - acc: 0.7352 - val_loss: 1.1516 - val_acc: 0.7217\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.0787 - acc: 0.7406 - val_loss: 1.3629 - val_acc: 0.6349\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.0661 - acc: 0.7469 - val_loss: 1.3721 - val_acc: 0.6501\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.0466 - acc: 0.7531 - val_loss: 1.4644 - val_acc: 0.6483\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.0390 - acc: 0.7608 - val_loss: 1.5135 - val_acc: 0.6078\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.0273 - acc: 0.7650 - val_loss: 1.4664 - val_acc: 0.6368\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.0197 - acc: 0.7673 - val_loss: 1.4304 - val_acc: 0.6485\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.0068 - acc: 0.7725 - val_loss: 1.2426 - val_acc: 0.7003\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.0037 - acc: 0.7744 - val_loss: 1.1012 - val_acc: 0.7420\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.9822 - acc: 0.7829 - val_loss: 1.1051 - val_acc: 0.7457\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.9805 - acc: 0.7819 - val_loss: 1.0986 - val_acc: 0.7410\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.8460 - acc: 0.8180 - val_loss: 1.0123 - val_acc: 0.7703\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.7888 - acc: 0.8255 - val_loss: 0.9209 - val_acc: 0.7868\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7701 - acc: 0.8295 - val_loss: 0.9603 - val_acc: 0.7672\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7719 - acc: 0.8273 - val_loss: 0.8842 - val_acc: 0.7934\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.7659 - acc: 0.8298 - val_loss: 0.8613 - val_acc: 0.8007\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7654 - acc: 0.8300 - val_loss: 0.9170 - val_acc: 0.7919\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7603 - acc: 0.8317 - val_loss: 0.8541 - val_acc: 0.8051\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7597 - acc: 0.8327 - val_loss: 0.9216 - val_acc: 0.7879\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7600 - acc: 0.8327 - val_loss: 0.8417 - val_acc: 0.8130\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7487 - acc: 0.8375 - val_loss: 0.9974 - val_acc: 0.7661\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7494 - acc: 0.8385 - val_loss: 0.7998 - val_acc: 0.8263\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7476 - acc: 0.8390 - val_loss: 0.8911 - val_acc: 0.7978\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7478 - acc: 0.8394 - val_loss: 0.8545 - val_acc: 0.8065\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7417 - acc: 0.8428 - val_loss: 0.9437 - val_acc: 0.7779\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7436 - acc: 0.8411 - val_loss: 1.0556 - val_acc: 0.7594\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7448 - acc: 0.8418 - val_loss: 0.9464 - val_acc: 0.7909\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7451 - acc: 0.8424 - val_loss: 0.8895 - val_acc: 0.8036\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.7387 - acc: 0.8437 - val_loss: 0.8806 - val_acc: 0.8023\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7356 - acc: 0.8458 - val_loss: 0.8447 - val_acc: 0.8108\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7354 - acc: 0.8463 - val_loss: 0.7993 - val_acc: 0.8269\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.6407 - acc: 0.8764 - val_loss: 0.7517 - val_acc: 0.8384\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.5959 - acc: 0.8850 - val_loss: 0.6766 - val_acc: 0.8555\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5817 - acc: 0.8835 - val_loss: 0.7396 - val_acc: 0.8373\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.5778 - acc: 0.8819 - val_loss: 0.7354 - val_acc: 0.8327\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5682 - acc: 0.8836 - val_loss: 0.7666 - val_acc: 0.8260\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5711 - acc: 0.8819 - val_loss: 0.6850 - val_acc: 0.8467\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5724 - acc: 0.8816 - val_loss: 0.7186 - val_acc: 0.8382\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5638 - acc: 0.8849 - val_loss: 0.7222 - val_acc: 0.8432\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5667 - acc: 0.8831 - val_loss: 0.7694 - val_acc: 0.8262\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5682 - acc: 0.8829 - val_loss: 0.6988 - val_acc: 0.8511\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5669 - acc: 0.8837 - val_loss: 0.7342 - val_acc: 0.8364\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5631 - acc: 0.8862 - val_loss: 0.6752 - val_acc: 0.8543\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.5646 - acc: 0.8844 - val_loss: 0.7145 - val_acc: 0.8412\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.5630 - acc: 0.8843 - val_loss: 0.7988 - val_acc: 0.8273\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.5622 - acc: 0.8864 - val_loss: 0.7393 - val_acc: 0.8387\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.5591 - acc: 0.8874 - val_loss: 0.9739 - val_acc: 0.7966\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5570 - acc: 0.8884 - val_loss: 0.7159 - val_acc: 0.8405\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5551 - acc: 0.8896 - val_loss: 0.7031 - val_acc: 0.8450\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.5572 - acc: 0.8903 - val_loss: 0.7678 - val_acc: 0.8359\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.5568 - acc: 0.8903 - val_loss: 0.7220 - val_acc: 0.8479\n",
      "Epoch 61/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 35s 89ms/step - loss: 0.4878 - acc: 0.9136 - val_loss: 0.6324 - val_acc: 0.8656\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4582 - acc: 0.9189 - val_loss: 0.6045 - val_acc: 0.8747\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4468 - acc: 0.9204 - val_loss: 0.6152 - val_acc: 0.8717\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4385 - acc: 0.9202 - val_loss: 0.6698 - val_acc: 0.8574\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4335 - acc: 0.9205 - val_loss: 0.6964 - val_acc: 0.8471\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4286 - acc: 0.9211 - val_loss: 0.7400 - val_acc: 0.8389\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4276 - acc: 0.9198 - val_loss: 0.6544 - val_acc: 0.8594\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4214 - acc: 0.9216 - val_loss: 0.6696 - val_acc: 0.8515\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4201 - acc: 0.9215 - val_loss: 0.6583 - val_acc: 0.8580\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4205 - acc: 0.9222 - val_loss: 0.6334 - val_acc: 0.8614\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.4250 - acc: 0.9194 - val_loss: 0.5892 - val_acc: 0.8747\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.4207 - acc: 0.9189 - val_loss: 0.7202 - val_acc: 0.8410\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4199 - acc: 0.9206 - val_loss: 0.6338 - val_acc: 0.8656\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4176 - acc: 0.9226 - val_loss: 0.7650 - val_acc: 0.8367\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4183 - acc: 0.9216 - val_loss: 0.6653 - val_acc: 0.8521\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4178 - acc: 0.9231 - val_loss: 0.6517 - val_acc: 0.8615\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4179 - acc: 0.9226 - val_loss: 0.6423 - val_acc: 0.8657\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4130 - acc: 0.9240 - val_loss: 0.6637 - val_acc: 0.8542\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4161 - acc: 0.9243 - val_loss: 0.6984 - val_acc: 0.8493\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4223 - acc: 0.9210 - val_loss: 0.6440 - val_acc: 0.8619\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3658 - acc: 0.9418 - val_loss: 0.5859 - val_acc: 0.8801\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3418 - acc: 0.9483 - val_loss: 0.5558 - val_acc: 0.8891\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3304 - acc: 0.9497 - val_loss: 0.5607 - val_acc: 0.8867\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3281 - acc: 0.9495 - val_loss: 0.5971 - val_acc: 0.8751\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3226 - acc: 0.9501 - val_loss: 0.5803 - val_acc: 0.8764\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3180 - acc: 0.9513 - val_loss: 0.5871 - val_acc: 0.8782\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3182 - acc: 0.9496 - val_loss: 0.6027 - val_acc: 0.8769\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3106 - acc: 0.9530 - val_loss: 0.5912 - val_acc: 0.8793\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3133 - acc: 0.9502 - val_loss: 0.5971 - val_acc: 0.8759\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.3088 - acc: 0.9529 - val_loss: 0.6164 - val_acc: 0.8701\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3053 - acc: 0.9529 - val_loss: 0.5979 - val_acc: 0.8764\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.3056 - acc: 0.9509 - val_loss: 0.5788 - val_acc: 0.8794\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3052 - acc: 0.9514 - val_loss: 0.5557 - val_acc: 0.8858\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3065 - acc: 0.9497 - val_loss: 0.5678 - val_acc: 0.8841\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3037 - acc: 0.9513 - val_loss: 0.5811 - val_acc: 0.8797\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3018 - acc: 0.9519 - val_loss: 0.5846 - val_acc: 0.8756\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3045 - acc: 0.9503 - val_loss: 0.6217 - val_acc: 0.8664\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3003 - acc: 0.9518 - val_loss: 0.5979 - val_acc: 0.8744\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3013 - acc: 0.9512 - val_loss: 0.5616 - val_acc: 0.8849\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2974 - acc: 0.9524 - val_loss: 0.5937 - val_acc: 0.8745\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.2675 - acc: 0.9630 - val_loss: 0.5470 - val_acc: 0.8898\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2528 - acc: 0.9681 - val_loss: 0.5399 - val_acc: 0.8926\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2470 - acc: 0.9690 - val_loss: 0.5417 - val_acc: 0.8904\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2412 - acc: 0.9711 - val_loss: 0.5517 - val_acc: 0.8901\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2381 - acc: 0.9707 - val_loss: 0.5671 - val_acc: 0.8877\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2379 - acc: 0.9702 - val_loss: 0.5382 - val_acc: 0.8908\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2315 - acc: 0.9721 - val_loss: 0.5779 - val_acc: 0.8877\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2301 - acc: 0.9719 - val_loss: 0.5503 - val_acc: 0.8896\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2275 - acc: 0.9728 - val_loss: 0.5860 - val_acc: 0.8862\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2231 - acc: 0.9734 - val_loss: 0.5439 - val_acc: 0.8939\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2275 - acc: 0.9712 - val_loss: 0.5955 - val_acc: 0.8808\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2241 - acc: 0.9722 - val_loss: 0.5707 - val_acc: 0.8863\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2204 - acc: 0.9731 - val_loss: 0.5749 - val_acc: 0.8855\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2173 - acc: 0.9733 - val_loss: 0.5359 - val_acc: 0.8921\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2208 - acc: 0.9721 - val_loss: 0.5790 - val_acc: 0.8847\n",
      "Epoch 116/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2160 - acc: 0.9731 - val_loss: 0.5534 - val_acc: 0.8876\n",
      "Epoch 117/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2144 - acc: 0.9727 - val_loss: 0.5759 - val_acc: 0.8832\n",
      "Epoch 118/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2136 - acc: 0.9736 - val_loss: 0.5658 - val_acc: 0.8843\n",
      "Epoch 119/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2164 - acc: 0.9722 - val_loss: 0.5714 - val_acc: 0.8860\n",
      "Epoch 120/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2176 - acc: 0.9716 - val_loss: 0.5596 - val_acc: 0.8832\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1983 - acc: 0.9787 - val_loss: 0.5421 - val_acc: 0.8912\n",
      "Epoch 122/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1900 - acc: 0.9815 - val_loss: 0.5333 - val_acc: 0.8925\n",
      "Epoch 123/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1853 - acc: 0.9831 - val_loss: 0.5330 - val_acc: 0.8947\n",
      "Epoch 124/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1813 - acc: 0.9832 - val_loss: 0.5499 - val_acc: 0.8937\n",
      "Epoch 125/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1780 - acc: 0.9847 - val_loss: 0.5361 - val_acc: 0.8952\n",
      "Epoch 126/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1778 - acc: 0.9841 - val_loss: 0.5627 - val_acc: 0.8913\n",
      "Epoch 127/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1743 - acc: 0.9858 - val_loss: 0.5393 - val_acc: 0.8953\n",
      "Epoch 128/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1737 - acc: 0.9849 - val_loss: 0.5666 - val_acc: 0.8911\n",
      "Epoch 129/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1709 - acc: 0.9851 - val_loss: 0.5410 - val_acc: 0.8952\n",
      "Epoch 130/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1676 - acc: 0.9858 - val_loss: 0.5620 - val_acc: 0.8935\n",
      "Epoch 131/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1686 - acc: 0.9861 - val_loss: 0.5472 - val_acc: 0.8929\n",
      "Epoch 132/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1674 - acc: 0.9853 - val_loss: 0.5595 - val_acc: 0.8910\n",
      "Epoch 133/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1675 - acc: 0.9853 - val_loss: 0.5674 - val_acc: 0.8921\n",
      "Epoch 134/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1651 - acc: 0.9858 - val_loss: 0.5745 - val_acc: 0.8925\n",
      "Epoch 135/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1648 - acc: 0.9856 - val_loss: 0.5574 - val_acc: 0.8932\n",
      "Epoch 136/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1632 - acc: 0.9857 - val_loss: 0.5790 - val_acc: 0.8910\n",
      "Epoch 137/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1624 - acc: 0.9857 - val_loss: 0.5751 - val_acc: 0.8886\n",
      "Epoch 138/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1614 - acc: 0.9854 - val_loss: 0.5543 - val_acc: 0.8936\n",
      "Epoch 139/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1620 - acc: 0.9854 - val_loss: 0.5871 - val_acc: 0.8879\n",
      "Epoch 140/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1604 - acc: 0.9856 - val_loss: 0.5448 - val_acc: 0.8955\n",
      "Epoch 141/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1553 - acc: 0.9876 - val_loss: 0.5558 - val_acc: 0.8896\n",
      "Epoch 142/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1504 - acc: 0.9891 - val_loss: 0.5446 - val_acc: 0.8927\n",
      "Epoch 143/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1496 - acc: 0.9888 - val_loss: 0.5514 - val_acc: 0.8942\n",
      "Epoch 00143: early stopping\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2890cf6807c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                         callbacks=callbacks_list)\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cifar10_dsodtiny_bb.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "lr_drop = 20\n",
    "\n",
    "#We create a checkpoint to save the best model and add an early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min')\n",
    "def lr_scheduler(epoch):\n",
    "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "reduce_lr = LearningRateScheduler(lr_scheduler)\n",
    "callbacks_list = [early_stop, reduce_lr]\n",
    "    \n",
    "\n",
    "print(model_name)\n",
    "    \n",
    "sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-6)\n",
    "classification_model.compile(optimizer = sgd, loss = \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "classification_model.fit_generator(\n",
    "                        datagen.flow(x_train, y_train, batch_size=128),\n",
    "                        epochs=300,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4,\n",
    "                        shuffle = True,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "classification_model.save_weights('cifar10_dsodtiny_bb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.save_weights('cifar10_dsodtiny_bb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = tinyDSOD_backbone((32,32,3), l2=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL.h5\n",
      "Epoch 1/300\n",
      "391/391 [==============================] - 51s 130ms/step - loss: 3.8584 - acc: 0.4109 - val_loss: 2.9361 - val_acc: 0.3079\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.9707 - acc: 0.5269 - val_loss: 1.7492 - val_acc: 0.5076\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.5557 - acc: 0.5759 - val_loss: 1.6910 - val_acc: 0.5308\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.4377 - acc: 0.6100 - val_loss: 1.5741 - val_acc: 0.5673\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.3779 - acc: 0.6395 - val_loss: 1.6342 - val_acc: 0.5567\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.3420 - acc: 0.6573 - val_loss: 1.5240 - val_acc: 0.5951\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.3236 - acc: 0.6735 - val_loss: 1.5803 - val_acc: 0.5808\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.2984 - acc: 0.6873 - val_loss: 2.7791 - val_acc: 0.3674\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.2867 - acc: 0.6923 - val_loss: 1.5624 - val_acc: 0.5933\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.2636 - acc: 0.7017 - val_loss: 2.0239 - val_acc: 0.4958\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.2437 - acc: 0.7100 - val_loss: 1.4036 - val_acc: 0.6498\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.2292 - acc: 0.7154 - val_loss: 1.9818 - val_acc: 0.5055\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.2153 - acc: 0.7199 - val_loss: 1.3855 - val_acc: 0.6627\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.2086 - acc: 0.7253 - val_loss: 1.5144 - val_acc: 0.6217\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.1975 - acc: 0.7278 - val_loss: 1.3429 - val_acc: 0.6713\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.1972 - acc: 0.7307 - val_loss: 1.4301 - val_acc: 0.6640\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.1788 - acc: 0.7365 - val_loss: 1.9675 - val_acc: 0.5413\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.1744 - acc: 0.7376 - val_loss: 1.5046 - val_acc: 0.6311\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.1651 - acc: 0.7424 - val_loss: 1.4743 - val_acc: 0.6538\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 1.1587 - acc: 0.7457 - val_loss: 1.5472 - val_acc: 0.6307\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.9938 - acc: 0.7864 - val_loss: 1.3826 - val_acc: 0.6606\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.9372 - acc: 0.7887 - val_loss: 1.0881 - val_acc: 0.7342\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.9224 - acc: 0.7923 - val_loss: 0.9882 - val_acc: 0.7724\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.9209 - acc: 0.7954 - val_loss: 1.2587 - val_acc: 0.6974\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.9267 - acc: 0.7931 - val_loss: 0.9461 - val_acc: 0.7864\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.9200 - acc: 0.7961 - val_loss: 1.0130 - val_acc: 0.7654\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.9179 - acc: 0.7990 - val_loss: 1.0889 - val_acc: 0.7468\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.9137 - acc: 0.8002 - val_loss: 1.2728 - val_acc: 0.6860\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.9174 - acc: 0.7998 - val_loss: 1.3243 - val_acc: 0.6865\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.9136 - acc: 0.8008 - val_loss: 1.2661 - val_acc: 0.6993\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.9021 - acc: 0.8034 - val_loss: 1.1018 - val_acc: 0.7347\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.9025 - acc: 0.8030 - val_loss: 1.0770 - val_acc: 0.7564\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.9092 - acc: 0.8044 - val_loss: 1.1323 - val_acc: 0.7293\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.9050 - acc: 0.8060 - val_loss: 1.3813 - val_acc: 0.6829\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.9063 - acc: 0.8046 - val_loss: 1.3186 - val_acc: 0.6782\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.8994 - acc: 0.8076 - val_loss: 1.5110 - val_acc: 0.6529\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.8904 - acc: 0.8116 - val_loss: 1.0462 - val_acc: 0.7657\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.9017 - acc: 0.8049 - val_loss: 1.1168 - val_acc: 0.7468\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.8987 - acc: 0.8081 - val_loss: 1.0475 - val_acc: 0.7618\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.8937 - acc: 0.8116 - val_loss: 0.9680 - val_acc: 0.7866\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7723 - acc: 0.8442 - val_loss: 0.8017 - val_acc: 0.8268\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7164 - acc: 0.8487 - val_loss: 0.8373 - val_acc: 0.8051\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.7071 - acc: 0.8476 - val_loss: 0.9449 - val_acc: 0.7732\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.7057 - acc: 0.8468 - val_loss: 0.9299 - val_acc: 0.7799\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.7091 - acc: 0.8465 - val_loss: 0.8288 - val_acc: 0.8084\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.7061 - acc: 0.8480 - val_loss: 0.8525 - val_acc: 0.8040\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.7013 - acc: 0.8513 - val_loss: 0.9678 - val_acc: 0.7715\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7056 - acc: 0.8502 - val_loss: 1.0134 - val_acc: 0.7643\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.7034 - acc: 0.8504 - val_loss: 1.1110 - val_acc: 0.7454\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.7011 - acc: 0.8516 - val_loss: 0.9227 - val_acc: 0.7824\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.7010 - acc: 0.8522 - val_loss: 0.9167 - val_acc: 0.7937\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.7017 - acc: 0.8538 - val_loss: 0.8462 - val_acc: 0.8075\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7028 - acc: 0.8532 - val_loss: 0.8454 - val_acc: 0.8106\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.6944 - acc: 0.8555 - val_loss: 0.8206 - val_acc: 0.8150\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.6994 - acc: 0.8535 - val_loss: 0.8347 - val_acc: 0.8110\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.7007 - acc: 0.8547 - val_loss: 1.0746 - val_acc: 0.7466\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.6963 - acc: 0.8568 - val_loss: 0.8265 - val_acc: 0.8205\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.6955 - acc: 0.8552 - val_loss: 0.8302 - val_acc: 0.8140\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.6959 - acc: 0.8562 - val_loss: 0.8615 - val_acc: 0.8059\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.6965 - acc: 0.8570 - val_loss: 0.7562 - val_acc: 0.8366\n",
      "Epoch 61/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 35s 89ms/step - loss: 0.6019 - acc: 0.8847 - val_loss: 0.6649 - val_acc: 0.8625\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5662 - acc: 0.8905 - val_loss: 0.6469 - val_acc: 0.8665\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5554 - acc: 0.8876 - val_loss: 0.6663 - val_acc: 0.8548\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5474 - acc: 0.8916 - val_loss: 0.7265 - val_acc: 0.8352\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.5509 - acc: 0.8879 - val_loss: 0.7775 - val_acc: 0.8201\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.5455 - acc: 0.8883 - val_loss: 0.7966 - val_acc: 0.8240\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.5408 - acc: 0.8893 - val_loss: 0.6826 - val_acc: 0.8471\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.5445 - acc: 0.8885 - val_loss: 0.6476 - val_acc: 0.8605\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.5397 - acc: 0.8892 - val_loss: 0.6993 - val_acc: 0.8460\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.5407 - acc: 0.8873 - val_loss: 0.6935 - val_acc: 0.8436\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5365 - acc: 0.8905 - val_loss: 0.7286 - val_acc: 0.8372\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5402 - acc: 0.8900 - val_loss: 0.7157 - val_acc: 0.8365\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5400 - acc: 0.8902 - val_loss: 0.6919 - val_acc: 0.8447\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.5403 - acc: 0.8907 - val_loss: 0.8410 - val_acc: 0.8067\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.5375 - acc: 0.8910 - val_loss: 0.6964 - val_acc: 0.8456\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.5393 - acc: 0.8913 - val_loss: 0.8281 - val_acc: 0.8060\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5348 - acc: 0.8943 - val_loss: 0.8577 - val_acc: 0.8150\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5398 - acc: 0.8926 - val_loss: 0.7379 - val_acc: 0.8364\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5352 - acc: 0.8946 - val_loss: 0.6835 - val_acc: 0.8498\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5337 - acc: 0.8935 - val_loss: 0.7115 - val_acc: 0.8403\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4768 - acc: 0.9125 - val_loss: 0.5961 - val_acc: 0.8742\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.4451 - acc: 0.9205 - val_loss: 0.5919 - val_acc: 0.8737\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.4341 - acc: 0.9216 - val_loss: 0.6136 - val_acc: 0.8684\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.4286 - acc: 0.9196 - val_loss: 0.6398 - val_acc: 0.8610\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.4260 - acc: 0.9221 - val_loss: 0.6342 - val_acc: 0.8592\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4188 - acc: 0.9215 - val_loss: 0.5829 - val_acc: 0.8744\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4192 - acc: 0.9207 - val_loss: 0.6770 - val_acc: 0.8499\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.4179 - acc: 0.9207 - val_loss: 0.5529 - val_acc: 0.8822\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4096 - acc: 0.9231 - val_loss: 0.5838 - val_acc: 0.8693\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.4161 - acc: 0.9197 - val_loss: 0.5751 - val_acc: 0.8757\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.4154 - acc: 0.9210 - val_loss: 0.6612 - val_acc: 0.8521\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.4131 - acc: 0.9209 - val_loss: 0.5827 - val_acc: 0.8741\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4120 - acc: 0.9218 - val_loss: 0.6056 - val_acc: 0.8676\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.4106 - acc: 0.9212 - val_loss: 0.5936 - val_acc: 0.8716\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4121 - acc: 0.9222 - val_loss: 0.6003 - val_acc: 0.8677\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.4151 - acc: 0.9197 - val_loss: 0.6375 - val_acc: 0.8587\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4112 - acc: 0.9220 - val_loss: 0.6881 - val_acc: 0.8460\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.4100 - acc: 0.9224 - val_loss: 0.6196 - val_acc: 0.8616\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4084 - acc: 0.9222 - val_loss: 0.6181 - val_acc: 0.8627\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4103 - acc: 0.9228 - val_loss: 0.5829 - val_acc: 0.8784\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3607 - acc: 0.9404 - val_loss: 0.5358 - val_acc: 0.8891\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3445 - acc: 0.9440 - val_loss: 0.5588 - val_acc: 0.8846\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.3321 - acc: 0.9460 - val_loss: 0.5530 - val_acc: 0.8863\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3265 - acc: 0.9481 - val_loss: 0.5425 - val_acc: 0.8884\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3249 - acc: 0.9472 - val_loss: 0.5455 - val_acc: 0.8862\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3173 - acc: 0.9493 - val_loss: 0.5630 - val_acc: 0.8810\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3158 - acc: 0.9492 - val_loss: 0.5372 - val_acc: 0.8898\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.3128 - acc: 0.9487 - val_loss: 0.6085 - val_acc: 0.8702\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3122 - acc: 0.9478 - val_loss: 0.5616 - val_acc: 0.8824\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3102 - acc: 0.9490 - val_loss: 0.5341 - val_acc: 0.8867\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 35s 91ms/step - loss: 0.3121 - acc: 0.9466 - val_loss: 0.5714 - val_acc: 0.8810\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3056 - acc: 0.9506 - val_loss: 0.5512 - val_acc: 0.8835\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.3124 - acc: 0.9465 - val_loss: 0.5797 - val_acc: 0.8760\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3049 - acc: 0.9482 - val_loss: 0.5612 - val_acc: 0.8786\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3028 - acc: 0.9495 - val_loss: 0.5867 - val_acc: 0.8744\n",
      "Epoch 116/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3073 - acc: 0.9480 - val_loss: 0.6284 - val_acc: 0.8599\n",
      "Epoch 117/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3037 - acc: 0.9490 - val_loss: 0.5738 - val_acc: 0.8750\n",
      "Epoch 118/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.3040 - acc: 0.9488 - val_loss: 0.5985 - val_acc: 0.8700\n",
      "Epoch 119/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3062 - acc: 0.9478 - val_loss: 0.5912 - val_acc: 0.8720\n",
      "Epoch 120/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.3055 - acc: 0.9478 - val_loss: 0.5584 - val_acc: 0.8838\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 35s 89ms/step - loss: 0.2751 - acc: 0.9584 - val_loss: 0.5180 - val_acc: 0.8901\n",
      "Epoch 122/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.2549 - acc: 0.9656 - val_loss: 0.5320 - val_acc: 0.8890\n",
      "Epoch 123/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2529 - acc: 0.9656 - val_loss: 0.5360 - val_acc: 0.8886\n",
      "Epoch 124/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.2468 - acc: 0.9668 - val_loss: 0.5481 - val_acc: 0.8848\n",
      "Epoch 125/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2432 - acc: 0.9669 - val_loss: 0.5532 - val_acc: 0.8868\n",
      "Epoch 126/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.2397 - acc: 0.9678 - val_loss: 0.5242 - val_acc: 0.8913\n",
      "Epoch 127/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.2362 - acc: 0.9692 - val_loss: 0.5546 - val_acc: 0.8831\n",
      "Epoch 128/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.2364 - acc: 0.9679 - val_loss: 0.5173 - val_acc: 0.8910\n",
      "Epoch 129/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2343 - acc: 0.9689 - val_loss: 0.5208 - val_acc: 0.8922\n",
      "Epoch 130/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.2275 - acc: 0.9709 - val_loss: 0.5237 - val_acc: 0.8942\n",
      "Epoch 131/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.2313 - acc: 0.9691 - val_loss: 0.5278 - val_acc: 0.8919\n",
      "Epoch 132/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.2273 - acc: 0.9695 - val_loss: 0.5183 - val_acc: 0.8948\n",
      "Epoch 133/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.2246 - acc: 0.9705 - val_loss: 0.5357 - val_acc: 0.8899\n",
      "Epoch 134/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.2270 - acc: 0.9687 - val_loss: 0.5226 - val_acc: 0.8914\n",
      "Epoch 135/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2252 - acc: 0.9691 - val_loss: 0.5503 - val_acc: 0.8853\n",
      "Epoch 136/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2250 - acc: 0.9688 - val_loss: 0.5263 - val_acc: 0.8882\n",
      "Epoch 137/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2216 - acc: 0.9701 - val_loss: 0.5531 - val_acc: 0.8840\n",
      "Epoch 138/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.2224 - acc: 0.9697 - val_loss: 0.5279 - val_acc: 0.8893\n",
      "Epoch 139/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2192 - acc: 0.9707 - val_loss: 0.5464 - val_acc: 0.8844\n",
      "Epoch 140/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.2225 - acc: 0.9689 - val_loss: 0.5538 - val_acc: 0.8845\n",
      "Epoch 141/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.2055 - acc: 0.9750 - val_loss: 0.5343 - val_acc: 0.8907\n",
      "Epoch 142/300\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.1956 - acc: 0.9785 - val_loss: 0.5147 - val_acc: 0.8957\n",
      "Epoch 143/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1928 - acc: 0.9788 - val_loss: 0.5207 - val_acc: 0.8909\n",
      "Epoch 144/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1881 - acc: 0.9803 - val_loss: 0.5185 - val_acc: 0.8933\n",
      "Epoch 145/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1874 - acc: 0.9801 - val_loss: 0.5167 - val_acc: 0.8941\n",
      "Epoch 146/300\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.1841 - acc: 0.9803 - val_loss: 0.5381 - val_acc: 0.8894\n",
      "Epoch 147/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1818 - acc: 0.9811 - val_loss: 0.5251 - val_acc: 0.8925\n",
      "Epoch 148/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1798 - acc: 0.9812 - val_loss: 0.5261 - val_acc: 0.8913\n",
      "Epoch 149/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1809 - acc: 0.9809 - val_loss: 0.5154 - val_acc: 0.8963\n",
      "Epoch 150/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1772 - acc: 0.9819 - val_loss: 0.5363 - val_acc: 0.8907\n",
      "Epoch 151/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1746 - acc: 0.9828 - val_loss: 0.5192 - val_acc: 0.8930\n",
      "Epoch 152/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1754 - acc: 0.9821 - val_loss: 0.5364 - val_acc: 0.8918\n",
      "Epoch 153/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1740 - acc: 0.9820 - val_loss: 0.5348 - val_acc: 0.8915\n",
      "Epoch 154/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1718 - acc: 0.9827 - val_loss: 0.5279 - val_acc: 0.8934\n",
      "Epoch 155/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1723 - acc: 0.9822 - val_loss: 0.5494 - val_acc: 0.8889\n",
      "Epoch 156/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1696 - acc: 0.9831 - val_loss: 0.5063 - val_acc: 0.8962\n",
      "Epoch 157/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1673 - acc: 0.9830 - val_loss: 0.5235 - val_acc: 0.8940\n",
      "Epoch 158/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1676 - acc: 0.9832 - val_loss: 0.5351 - val_acc: 0.8916\n",
      "Epoch 159/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1686 - acc: 0.9831 - val_loss: 0.5459 - val_acc: 0.8911\n",
      "Epoch 160/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1694 - acc: 0.9816 - val_loss: 0.5259 - val_acc: 0.8901\n",
      "Epoch 161/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1592 - acc: 0.9858 - val_loss: 0.5151 - val_acc: 0.8947\n",
      "Epoch 162/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1581 - acc: 0.9862 - val_loss: 0.5327 - val_acc: 0.8909\n",
      "Epoch 163/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1549 - acc: 0.9869 - val_loss: 0.5162 - val_acc: 0.8955\n",
      "Epoch 164/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1526 - acc: 0.9874 - val_loss: 0.5201 - val_acc: 0.8936\n",
      "Epoch 165/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1540 - acc: 0.9869 - val_loss: 0.5174 - val_acc: 0.8949\n",
      "Epoch 166/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1517 - acc: 0.9870 - val_loss: 0.5133 - val_acc: 0.8972\n",
      "Epoch 167/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1497 - acc: 0.9880 - val_loss: 0.5157 - val_acc: 0.8943\n",
      "Epoch 168/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1483 - acc: 0.9884 - val_loss: 0.5086 - val_acc: 0.8968\n",
      "Epoch 169/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1463 - acc: 0.9889 - val_loss: 0.5197 - val_acc: 0.8954\n",
      "Epoch 170/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1468 - acc: 0.9885 - val_loss: 0.5234 - val_acc: 0.8926\n",
      "Epoch 171/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1459 - acc: 0.9889 - val_loss: 0.5190 - val_acc: 0.8949\n",
      "Epoch 172/300\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.1441 - acc: 0.9892 - val_loss: 0.5089 - val_acc: 0.8966\n",
      "Epoch 173/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1441 - acc: 0.9891 - val_loss: 0.5217 - val_acc: 0.8945\n",
      "Epoch 174/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1431 - acc: 0.9885 - val_loss: 0.5274 - val_acc: 0.8930\n",
      "Epoch 175/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1421 - acc: 0.9897 - val_loss: 0.5103 - val_acc: 0.8981\n",
      "Epoch 176/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1412 - acc: 0.9892 - val_loss: 0.5306 - val_acc: 0.8911\n",
      "Epoch 177/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1403 - acc: 0.9895 - val_loss: 0.5186 - val_acc: 0.8950\n",
      "Epoch 178/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1420 - acc: 0.9886 - val_loss: 0.5211 - val_acc: 0.8962\n",
      "Epoch 179/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1398 - acc: 0.9891 - val_loss: 0.5171 - val_acc: 0.8974\n",
      "Epoch 180/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1379 - acc: 0.9894 - val_loss: 0.5252 - val_acc: 0.8929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1360 - acc: 0.9905 - val_loss: 0.5138 - val_acc: 0.8973\n",
      "Epoch 182/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1345 - acc: 0.9909 - val_loss: 0.5103 - val_acc: 0.8981\n",
      "Epoch 183/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1341 - acc: 0.9910 - val_loss: 0.5174 - val_acc: 0.8968\n",
      "Epoch 184/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1352 - acc: 0.9904 - val_loss: 0.5208 - val_acc: 0.8960\n",
      "Epoch 185/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1332 - acc: 0.9916 - val_loss: 0.5147 - val_acc: 0.8969\n",
      "Epoch 186/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1321 - acc: 0.9915 - val_loss: 0.5146 - val_acc: 0.8969\n",
      "Epoch 187/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1308 - acc: 0.9916 - val_loss: 0.5224 - val_acc: 0.8964\n",
      "Epoch 188/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1309 - acc: 0.9913 - val_loss: 0.5183 - val_acc: 0.8975\n",
      "Epoch 189/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1309 - acc: 0.9915 - val_loss: 0.5149 - val_acc: 0.8966\n",
      "Epoch 190/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1311 - acc: 0.9916 - val_loss: 0.5186 - val_acc: 0.8985\n",
      "Epoch 191/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1283 - acc: 0.9928 - val_loss: 0.5176 - val_acc: 0.8981\n",
      "Epoch 192/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1291 - acc: 0.9923 - val_loss: 0.5152 - val_acc: 0.8984\n",
      "Epoch 193/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1304 - acc: 0.9914 - val_loss: 0.5221 - val_acc: 0.8971\n",
      "Epoch 194/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1290 - acc: 0.9917 - val_loss: 0.5238 - val_acc: 0.8965\n",
      "Epoch 195/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1294 - acc: 0.9916 - val_loss: 0.5200 - val_acc: 0.8971\n",
      "Epoch 196/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1282 - acc: 0.9917 - val_loss: 0.5199 - val_acc: 0.8981\n",
      "Epoch 197/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1284 - acc: 0.9921 - val_loss: 0.5135 - val_acc: 0.8981\n",
      "Epoch 198/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1263 - acc: 0.9928 - val_loss: 0.5212 - val_acc: 0.8966\n",
      "Epoch 199/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1263 - acc: 0.9924 - val_loss: 0.5080 - val_acc: 0.9001\n",
      "Epoch 200/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1261 - acc: 0.9922 - val_loss: 0.5178 - val_acc: 0.8976\n",
      "Epoch 201/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1238 - acc: 0.9930 - val_loss: 0.5189 - val_acc: 0.8977\n",
      "Epoch 202/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1261 - acc: 0.9918 - val_loss: 0.5212 - val_acc: 0.8966\n",
      "Epoch 203/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1245 - acc: 0.9929 - val_loss: 0.5201 - val_acc: 0.8959\n",
      "Epoch 204/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1236 - acc: 0.9930 - val_loss: 0.5200 - val_acc: 0.8959\n",
      "Epoch 205/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1245 - acc: 0.9926 - val_loss: 0.5184 - val_acc: 0.8972\n",
      "Epoch 206/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1241 - acc: 0.9926 - val_loss: 0.5173 - val_acc: 0.8973\n",
      "Epoch 207/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1240 - acc: 0.9929 - val_loss: 0.5155 - val_acc: 0.8979\n",
      "Epoch 208/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1234 - acc: 0.9926 - val_loss: 0.5153 - val_acc: 0.8987\n",
      "Epoch 209/300\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.1242 - acc: 0.9929 - val_loss: 0.5174 - val_acc: 0.8974\n",
      "Epoch 210/300\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.1232 - acc: 0.9929 - val_loss: 0.5213 - val_acc: 0.8969\n",
      "Epoch 211/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1221 - acc: 0.9933 - val_loss: 0.5211 - val_acc: 0.8960\n",
      "Epoch 212/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1237 - acc: 0.9926 - val_loss: 0.5205 - val_acc: 0.8977\n",
      "Epoch 213/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1221 - acc: 0.9932 - val_loss: 0.5199 - val_acc: 0.8981\n",
      "Epoch 214/300\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.1236 - acc: 0.9925 - val_loss: 0.5174 - val_acc: 0.8977\n",
      "Epoch 215/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1224 - acc: 0.9929 - val_loss: 0.5144 - val_acc: 0.8990\n",
      "Epoch 216/300\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1216 - acc: 0.9930 - val_loss: 0.5130 - val_acc: 0.8993\n",
      "Epoch 00216: early stopping\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "lr_drop = 20\n",
    "\n",
    "#We create a checkpoint to save the best model and add an early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=60, verbose=1, mode='min')\n",
    "def lr_scheduler(epoch):\n",
    "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "reduce_lr = LearningRateScheduler(lr_scheduler)\n",
    "callbacks_list = [early_stop, reduce_lr]\n",
    "    \n",
    "\n",
    "print(model_name)\n",
    "    \n",
    "sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-6)\n",
    "classification_model.compile(optimizer = sgd, loss = \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "classification_model.fit_generator(\n",
    "                        datagen.flow(x_train, y_train, batch_size=128),\n",
    "                        epochs=300,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4,\n",
    "                        shuffle = True,\n",
    "                        callbacks=callbacks_list)\n",
    "\n",
    "classification_model.save_weights('cifar10_dsodtiny_bb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.load_weights('cifar10_dsodtiny_bb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL.h5\n",
      "Epoch 1/300\n",
      "391/391 [==============================] - 64s 164ms/step - loss: 2.8208 - acc: 0.4359 - val_loss: 2.5351 - val_acc: 0.4325\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 1.8887 - acc: 0.5795 - val_loss: 2.0169 - val_acc: 0.5279\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 1.5068 - acc: 0.6316 - val_loss: 1.5165 - val_acc: 0.6109\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 1.3179 - acc: 0.6588 - val_loss: 1.4858 - val_acc: 0.6007\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 1.2052 - acc: 0.6905 - val_loss: 1.2353 - val_acc: 0.6851\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 1.1382 - acc: 0.7108 - val_loss: 1.2457 - val_acc: 0.6818\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 1.0888 - acc: 0.7306 - val_loss: 2.4875 - val_acc: 0.4352\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 1.0759 - acc: 0.7369 - val_loss: 1.5260 - val_acc: 0.6104\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 1.0442 - acc: 0.7496 - val_loss: 1.6300 - val_acc: 0.6053\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 1.0325 - acc: 0.7565 - val_loss: 1.1336 - val_acc: 0.7256\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 1.0159 - acc: 0.7634 - val_loss: 1.1247 - val_acc: 0.7296\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 1.0049 - acc: 0.7671 - val_loss: 1.3635 - val_acc: 0.6789\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.9902 - acc: 0.7746 - val_loss: 1.2136 - val_acc: 0.7062\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.9837 - acc: 0.7764 - val_loss: 1.3510 - val_acc: 0.6694\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.9777 - acc: 0.7820 - val_loss: 1.2806 - val_acc: 0.6949\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.9698 - acc: 0.7836 - val_loss: 1.1573 - val_acc: 0.7159\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.9577 - acc: 0.7876 - val_loss: 1.1567 - val_acc: 0.7204\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.9602 - acc: 0.7889 - val_loss: 1.5683 - val_acc: 0.6235\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.9472 - acc: 0.7926 - val_loss: 1.4057 - val_acc: 0.6605\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.9374 - acc: 0.7982 - val_loss: 1.2774 - val_acc: 0.6948\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.8153 - acc: 0.8286 - val_loss: 1.0542 - val_acc: 0.7481\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7616 - acc: 0.8358 - val_loss: 0.8943 - val_acc: 0.7949\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7476 - acc: 0.8356 - val_loss: 1.1108 - val_acc: 0.7367\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7449 - acc: 0.8353 - val_loss: 0.8811 - val_acc: 0.7910\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7430 - acc: 0.8381 - val_loss: 1.0152 - val_acc: 0.7607\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.7372 - acc: 0.8377 - val_loss: 0.8921 - val_acc: 0.7862\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.7405 - acc: 0.8383 - val_loss: 0.8529 - val_acc: 0.8034\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.7387 - acc: 0.8380 - val_loss: 0.9070 - val_acc: 0.7894\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7345 - acc: 0.8432 - val_loss: 1.0234 - val_acc: 0.7621\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7365 - acc: 0.8405 - val_loss: 0.9769 - val_acc: 0.7744\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7293 - acc: 0.8445 - val_loss: 0.8985 - val_acc: 0.7853\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7305 - acc: 0.8424 - val_loss: 1.1065 - val_acc: 0.7492\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7256 - acc: 0.8474 - val_loss: 0.9064 - val_acc: 0.7867\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.7256 - acc: 0.8466 - val_loss: 0.9262 - val_acc: 0.7882\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7231 - acc: 0.8473 - val_loss: 0.9332 - val_acc: 0.7837\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.7248 - acc: 0.8483 - val_loss: 0.7638 - val_acc: 0.8414\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.7192 - acc: 0.8496 - val_loss: 0.8948 - val_acc: 0.7984\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7187 - acc: 0.8519 - val_loss: 1.0456 - val_acc: 0.7590\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.7226 - acc: 0.8485 - val_loss: 1.1111 - val_acc: 0.7522\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7207 - acc: 0.8504 - val_loss: 0.8260 - val_acc: 0.8173\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.6286 - acc: 0.8777 - val_loss: 0.6780 - val_acc: 0.8623\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5874 - acc: 0.8860 - val_loss: 0.6540 - val_acc: 0.8655\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5674 - acc: 0.8868 - val_loss: 0.8943 - val_acc: 0.7979\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5733 - acc: 0.8812 - val_loss: 0.7186 - val_acc: 0.8358\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5628 - acc: 0.8852 - val_loss: 0.7067 - val_acc: 0.8419\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5591 - acc: 0.8852 - val_loss: 0.7094 - val_acc: 0.8428\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5596 - acc: 0.8838 - val_loss: 0.7506 - val_acc: 0.8263\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.5604 - acc: 0.8838 - val_loss: 0.7779 - val_acc: 0.8248\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5544 - acc: 0.8872 - val_loss: 0.7352 - val_acc: 0.8351\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5530 - acc: 0.8864 - val_loss: 0.8677 - val_acc: 0.8016\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5514 - acc: 0.8872 - val_loss: 0.7058 - val_acc: 0.8429\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5532 - acc: 0.8874 - val_loss: 0.6782 - val_acc: 0.8513\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5544 - acc: 0.8876 - val_loss: 0.6819 - val_acc: 0.8506\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.5506 - acc: 0.8893 - val_loss: 0.7106 - val_acc: 0.8410\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.5540 - acc: 0.8878 - val_loss: 0.7224 - val_acc: 0.8374\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5473 - acc: 0.8913 - val_loss: 0.7437 - val_acc: 0.8344\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5531 - acc: 0.8887 - val_loss: 0.7101 - val_acc: 0.8454\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5474 - acc: 0.8917 - val_loss: 0.7939 - val_acc: 0.8230\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5525 - acc: 0.8907 - val_loss: 0.6877 - val_acc: 0.8567\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5465 - acc: 0.8928 - val_loss: 0.6598 - val_acc: 0.8611\n",
      "Epoch 61/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4835 - acc: 0.9125 - val_loss: 0.5928 - val_acc: 0.8801\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4512 - acc: 0.9206 - val_loss: 0.6101 - val_acc: 0.8720\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.4430 - acc: 0.9202 - val_loss: 0.6124 - val_acc: 0.8725\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4340 - acc: 0.9212 - val_loss: 0.6156 - val_acc: 0.8706\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4287 - acc: 0.9225 - val_loss: 0.6530 - val_acc: 0.8618\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4213 - acc: 0.9228 - val_loss: 0.6451 - val_acc: 0.8582\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4230 - acc: 0.9212 - val_loss: 0.6045 - val_acc: 0.8685\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4201 - acc: 0.9223 - val_loss: 0.7016 - val_acc: 0.8432\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4204 - acc: 0.9200 - val_loss: 0.6220 - val_acc: 0.8656\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4201 - acc: 0.9200 - val_loss: 0.6308 - val_acc: 0.8631\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4169 - acc: 0.9216 - val_loss: 0.6134 - val_acc: 0.8688\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4146 - acc: 0.9212 - val_loss: 0.6568 - val_acc: 0.8539\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4126 - acc: 0.9225 - val_loss: 0.5789 - val_acc: 0.8745\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4142 - acc: 0.9217 - val_loss: 0.5797 - val_acc: 0.8763\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4125 - acc: 0.9232 - val_loss: 0.5766 - val_acc: 0.8750\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4140 - acc: 0.9234 - val_loss: 0.5664 - val_acc: 0.8823\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4114 - acc: 0.9223 - val_loss: 0.6228 - val_acc: 0.8613\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4110 - acc: 0.9229 - val_loss: 0.6290 - val_acc: 0.8664\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.4137 - acc: 0.9224 - val_loss: 0.6398 - val_acc: 0.8629\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4097 - acc: 0.9247 - val_loss: 0.5824 - val_acc: 0.8776\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.3586 - acc: 0.9427 - val_loss: 0.5213 - val_acc: 0.8957\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.3374 - acc: 0.9480 - val_loss: 0.5988 - val_acc: 0.8759\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3267 - acc: 0.9501 - val_loss: 0.5534 - val_acc: 0.8886\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.3224 - acc: 0.9506 - val_loss: 0.5751 - val_acc: 0.8814\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.3202 - acc: 0.9508 - val_loss: 0.5548 - val_acc: 0.8846\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.3130 - acc: 0.9521 - val_loss: 0.5415 - val_acc: 0.8913\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3162 - acc: 0.9497 - val_loss: 0.5475 - val_acc: 0.8841\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.3086 - acc: 0.9523 - val_loss: 0.5894 - val_acc: 0.8787\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.3077 - acc: 0.9514 - val_loss: 0.5440 - val_acc: 0.8854\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3075 - acc: 0.9500 - val_loss: 0.5614 - val_acc: 0.8817\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.3052 - acc: 0.9508 - val_loss: 0.5779 - val_acc: 0.8777\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3094 - acc: 0.9500 - val_loss: 0.5624 - val_acc: 0.8825\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2979 - acc: 0.9537 - val_loss: 0.5533 - val_acc: 0.8844\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.3010 - acc: 0.9511 - val_loss: 0.5615 - val_acc: 0.8835\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.3018 - acc: 0.9507 - val_loss: 0.5774 - val_acc: 0.8772\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2999 - acc: 0.9507 - val_loss: 0.6177 - val_acc: 0.8707\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2992 - acc: 0.9513 - val_loss: 0.5599 - val_acc: 0.8832\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2989 - acc: 0.9522 - val_loss: 0.6033 - val_acc: 0.8702\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2957 - acc: 0.9528 - val_loss: 0.5443 - val_acc: 0.8877\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2982 - acc: 0.9510 - val_loss: 0.5912 - val_acc: 0.8743\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2635 - acc: 0.9637 - val_loss: 0.5540 - val_acc: 0.8829\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2493 - acc: 0.9686 - val_loss: 0.5449 - val_acc: 0.8892\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2420 - acc: 0.9700 - val_loss: 0.5406 - val_acc: 0.8887\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2368 - acc: 0.9712 - val_loss: 0.5498 - val_acc: 0.8882\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2334 - acc: 0.9715 - val_loss: 0.5586 - val_acc: 0.8882\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2324 - acc: 0.9717 - val_loss: 0.5460 - val_acc: 0.8922\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2284 - acc: 0.9725 - val_loss: 0.5648 - val_acc: 0.8862\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2281 - acc: 0.9720 - val_loss: 0.5442 - val_acc: 0.8887\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2263 - acc: 0.9722 - val_loss: 0.5530 - val_acc: 0.8862\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2262 - acc: 0.9720 - val_loss: 0.5661 - val_acc: 0.8856\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.2203 - acc: 0.9734 - val_loss: 0.5346 - val_acc: 0.8937\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.2191 - acc: 0.9734 - val_loss: 0.5220 - val_acc: 0.8941\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.2205 - acc: 0.9723 - val_loss: 0.5513 - val_acc: 0.8901\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 46s 119ms/step - loss: 0.2212 - acc: 0.9712 - val_loss: 0.5563 - val_acc: 0.8867\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 43s 111ms/step - loss: 0.2180 - acc: 0.9726 - val_loss: 0.5388 - val_acc: 0.8883\n",
      "Epoch 116/300\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.2180 - acc: 0.9710 - val_loss: 0.5520 - val_acc: 0.8867\n",
      "Epoch 117/300\n",
      "391/391 [==============================] - 56s 143ms/step - loss: 0.2156 - acc: 0.9719 - val_loss: 0.5753 - val_acc: 0.8835\n",
      "Epoch 118/300\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.2156 - acc: 0.9722 - val_loss: 0.5705 - val_acc: 0.8811\n",
      "Epoch 119/300\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.2117 - acc: 0.9733 - val_loss: 0.5720 - val_acc: 0.8863\n",
      "Epoch 120/300\n",
      "391/391 [==============================] - 47s 121ms/step - loss: 0.2106 - acc: 0.9730 - val_loss: 0.5667 - val_acc: 0.8861\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 46s 117ms/step - loss: 0.1972 - acc: 0.9772 - val_loss: 0.5164 - val_acc: 0.8980\n",
      "Epoch 122/300\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1870 - acc: 0.9815 - val_loss: 0.5216 - val_acc: 0.8932\n",
      "Epoch 123/300\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1822 - acc: 0.9830 - val_loss: 0.5299 - val_acc: 0.8940\n",
      "Epoch 124/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1806 - acc: 0.9831 - val_loss: 0.5107 - val_acc: 0.8990\n",
      "Epoch 125/300\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1745 - acc: 0.9848 - val_loss: 0.5135 - val_acc: 0.8981\n",
      "Epoch 126/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1750 - acc: 0.9842 - val_loss: 0.5229 - val_acc: 0.8965\n",
      "Epoch 127/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1716 - acc: 0.9851 - val_loss: 0.5377 - val_acc: 0.8921\n",
      "Epoch 128/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1711 - acc: 0.9847 - val_loss: 0.5381 - val_acc: 0.8911\n",
      "Epoch 129/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1698 - acc: 0.9849 - val_loss: 0.5210 - val_acc: 0.8986\n",
      "Epoch 130/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1684 - acc: 0.9850 - val_loss: 0.5543 - val_acc: 0.8904\n",
      "Epoch 131/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1679 - acc: 0.9853 - val_loss: 0.5234 - val_acc: 0.8959\n",
      "Epoch 132/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1664 - acc: 0.9855 - val_loss: 0.5104 - val_acc: 0.9018\n",
      "Epoch 133/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1641 - acc: 0.9860 - val_loss: 0.5508 - val_acc: 0.8918\n",
      "Epoch 134/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1666 - acc: 0.9844 - val_loss: 0.5289 - val_acc: 0.8948\n",
      "Epoch 135/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1622 - acc: 0.9855 - val_loss: 0.5176 - val_acc: 0.8979\n",
      "Epoch 136/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1618 - acc: 0.9858 - val_loss: 0.5243 - val_acc: 0.9004\n",
      "Epoch 137/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1617 - acc: 0.9851 - val_loss: 0.5344 - val_acc: 0.8921\n",
      "Epoch 138/300\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1592 - acc: 0.9864 - val_loss: 0.5196 - val_acc: 0.8977\n",
      "Epoch 139/300\n",
      "391/391 [==============================] - 47s 119ms/step - loss: 0.1597 - acc: 0.9855 - val_loss: 0.5404 - val_acc: 0.8950\n",
      "Epoch 140/300\n",
      "391/391 [==============================] - 46s 119ms/step - loss: 0.1595 - acc: 0.9851 - val_loss: 0.5346 - val_acc: 0.8956\n",
      "Epoch 141/300\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 0.1529 - acc: 0.9877 - val_loss: 0.5218 - val_acc: 0.8978\n",
      "Epoch 142/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.1500 - acc: 0.9883 - val_loss: 0.5068 - val_acc: 0.9007\n",
      "Epoch 143/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1476 - acc: 0.9896 - val_loss: 0.5078 - val_acc: 0.9016\n",
      "Epoch 144/300\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1451 - acc: 0.9902 - val_loss: 0.5136 - val_acc: 0.8983\n",
      "Epoch 145/300\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.1432 - acc: 0.9908 - val_loss: 0.5133 - val_acc: 0.8987\n",
      "Epoch 146/300\n",
      "391/391 [==============================] - 50s 127ms/step - loss: 0.1414 - acc: 0.9910 - val_loss: 0.5245 - val_acc: 0.8976\n",
      "Epoch 147/300\n",
      "391/391 [==============================] - 47s 119ms/step - loss: 0.1421 - acc: 0.9905 - val_loss: 0.5228 - val_acc: 0.9005\n",
      "Epoch 148/300\n",
      "391/391 [==============================] - 49s 126ms/step - loss: 0.1389 - acc: 0.9913 - val_loss: 0.5291 - val_acc: 0.8999\n",
      "Epoch 149/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.1402 - acc: 0.9910 - val_loss: 0.5205 - val_acc: 0.9008\n",
      "Epoch 150/300\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.1403 - acc: 0.9901 - val_loss: 0.5149 - val_acc: 0.9008\n",
      "Epoch 151/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.1387 - acc: 0.9903 - val_loss: 0.5234 - val_acc: 0.8985\n",
      "Epoch 152/300\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.1356 - acc: 0.9918 - val_loss: 0.5235 - val_acc: 0.8986\n",
      "Epoch 153/300\n",
      "391/391 [==============================] - 48s 122ms/step - loss: 0.1373 - acc: 0.9908 - val_loss: 0.5258 - val_acc: 0.8986\n",
      "Epoch 154/300\n",
      "391/391 [==============================] - 48s 122ms/step - loss: 0.1365 - acc: 0.9912 - val_loss: 0.5304 - val_acc: 0.8994\n",
      "Epoch 155/300\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.1367 - acc: 0.9907 - val_loss: 0.5302 - val_acc: 0.8966\n",
      "Epoch 156/300\n",
      "391/391 [==============================] - 48s 122ms/step - loss: 0.1348 - acc: 0.9915 - val_loss: 0.5357 - val_acc: 0.8979\n",
      "Epoch 157/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.1322 - acc: 0.9920 - val_loss: 0.5250 - val_acc: 0.8983\n",
      "Epoch 158/300\n",
      "391/391 [==============================] - 49s 124ms/step - loss: 0.1336 - acc: 0.9916 - val_loss: 0.5254 - val_acc: 0.9013\n",
      "Epoch 159/300\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.1333 - acc: 0.9917 - val_loss: 0.5421 - val_acc: 0.8949\n",
      "Epoch 160/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.1331 - acc: 0.9913 - val_loss: 0.5217 - val_acc: 0.9004\n",
      "Epoch 161/300\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.1267 - acc: 0.9934 - val_loss: 0.5200 - val_acc: 0.9011\n",
      "Epoch 162/300\n",
      "391/391 [==============================] - 47s 121ms/step - loss: 0.1294 - acc: 0.9925 - val_loss: 0.5245 - val_acc: 0.8981\n",
      "Epoch 163/300\n",
      "391/391 [==============================] - 48s 122ms/step - loss: 0.1283 - acc: 0.9926 - val_loss: 0.5194 - val_acc: 0.9006\n",
      "Epoch 164/300\n",
      "391/391 [==============================] - 49s 125ms/step - loss: 0.1279 - acc: 0.9926 - val_loss: 0.5141 - val_acc: 0.9023\n",
      "Epoch 165/300\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.1257 - acc: 0.9934 - val_loss: 0.5182 - val_acc: 0.9008\n",
      "Epoch 166/300\n",
      "391/391 [==============================] - 49s 124ms/step - loss: 0.1262 - acc: 0.9933 - val_loss: 0.5201 - val_acc: 0.8991\n",
      "Epoch 167/300\n",
      "391/391 [==============================] - 50s 127ms/step - loss: 0.1254 - acc: 0.9933 - val_loss: 0.5221 - val_acc: 0.8990\n",
      "Epoch 168/300\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.1245 - acc: 0.9936 - val_loss: 0.5247 - val_acc: 0.9006\n",
      "Epoch 169/300\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.1241 - acc: 0.9932 - val_loss: 0.5290 - val_acc: 0.8987\n",
      "Epoch 170/300\n",
      "391/391 [==============================] - 49s 125ms/step - loss: 0.1246 - acc: 0.9933 - val_loss: 0.5196 - val_acc: 0.9000\n",
      "Epoch 171/300\n",
      "391/391 [==============================] - 54s 138ms/step - loss: 0.1234 - acc: 0.9939 - val_loss: 0.5182 - val_acc: 0.9002\n",
      "Epoch 172/300\n",
      "391/391 [==============================] - 52s 132ms/step - loss: 0.1222 - acc: 0.9944 - val_loss: 0.5251 - val_acc: 0.8984\n",
      "Epoch 173/300\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.1225 - acc: 0.9936 - val_loss: 0.5224 - val_acc: 0.9019\n",
      "Epoch 174/300\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 0.1216 - acc: 0.9942 - val_loss: 0.5336 - val_acc: 0.8991\n",
      "Epoch 175/300\n",
      "391/391 [==============================] - 52s 134ms/step - loss: 0.1205 - acc: 0.9944 - val_loss: 0.5339 - val_acc: 0.8980\n",
      "Epoch 176/300\n",
      "391/391 [==============================] - 53s 136ms/step - loss: 0.1206 - acc: 0.9944 - val_loss: 0.5273 - val_acc: 0.8992\n",
      "Epoch 177/300\n",
      "391/391 [==============================] - 54s 138ms/step - loss: 0.1205 - acc: 0.9944 - val_loss: 0.5215 - val_acc: 0.8995\n",
      "Epoch 178/300\n",
      "391/391 [==============================] - 53s 136ms/step - loss: 0.1205 - acc: 0.9941 - val_loss: 0.5254 - val_acc: 0.8993\n",
      "Epoch 179/300\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.1204 - acc: 0.9940 - val_loss: 0.5256 - val_acc: 0.9006\n",
      "Epoch 180/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1200 - acc: 0.9937 - val_loss: 0.5231 - val_acc: 0.8992\n",
      "Epoch 181/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1191 - acc: 0.9941 - val_loss: 0.5236 - val_acc: 0.8989\n",
      "Epoch 182/300\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1172 - acc: 0.9948 - val_loss: 0.5257 - val_acc: 0.8990\n",
      "Epoch 183/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1187 - acc: 0.9941 - val_loss: 0.5241 - val_acc: 0.9002\n",
      "Epoch 184/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1181 - acc: 0.9944 - val_loss: 0.5263 - val_acc: 0.8998\n",
      "Epoch 185/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1172 - acc: 0.9950 - val_loss: 0.5239 - val_acc: 0.9003\n",
      "Epoch 186/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1174 - acc: 0.9945 - val_loss: 0.5272 - val_acc: 0.9010\n",
      "Epoch 187/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1175 - acc: 0.9945 - val_loss: 0.5276 - val_acc: 0.9014\n",
      "Epoch 188/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1164 - acc: 0.9954 - val_loss: 0.5304 - val_acc: 0.8997\n",
      "Epoch 189/300\n",
      "391/391 [==============================] - 44s 114ms/step - loss: 0.1172 - acc: 0.9945 - val_loss: 0.5248 - val_acc: 0.9008\n",
      "Epoch 190/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1157 - acc: 0.9951 - val_loss: 0.5254 - val_acc: 0.9012\n",
      "Epoch 191/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1167 - acc: 0.9949 - val_loss: 0.5270 - val_acc: 0.8993\n",
      "Epoch 192/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1163 - acc: 0.9948 - val_loss: 0.5237 - val_acc: 0.9024\n",
      "Epoch 193/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1144 - acc: 0.9952 - val_loss: 0.5234 - val_acc: 0.9020\n",
      "Epoch 194/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1161 - acc: 0.9947 - val_loss: 0.5223 - val_acc: 0.9010\n",
      "Epoch 195/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1148 - acc: 0.9957 - val_loss: 0.5254 - val_acc: 0.9014\n",
      "Epoch 196/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1145 - acc: 0.9954 - val_loss: 0.5258 - val_acc: 0.9019\n",
      "Epoch 197/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1132 - acc: 0.9958 - val_loss: 0.5332 - val_acc: 0.8990\n",
      "Epoch 198/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1144 - acc: 0.9951 - val_loss: 0.5304 - val_acc: 0.9014\n",
      "Epoch 199/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1141 - acc: 0.9955 - val_loss: 0.5305 - val_acc: 0.9001\n",
      "Epoch 200/300\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1141 - acc: 0.9952 - val_loss: 0.5304 - val_acc: 0.8986\n",
      "Epoch 201/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1142 - acc: 0.9955 - val_loss: 0.5281 - val_acc: 0.9002\n",
      "Epoch 202/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1142 - acc: 0.9950 - val_loss: 0.5272 - val_acc: 0.9009\n",
      "Epoch 00202: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f101b3cbf60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model = tinyDSOD_backbone((32,32,3), l2=5e-4)\n",
    "\n",
    "learning_rate = 0.1\n",
    "lr_drop = 20\n",
    "\n",
    "#We create a checkpoint to save the best model and add an early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=60, verbose=1, mode='min')\n",
    "def lr_scheduler(epoch):\n",
    "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "reduce_lr = LearningRateScheduler(lr_scheduler)\n",
    "callbacks_list = [early_stop, reduce_lr]\n",
    "    \n",
    "\n",
    "print(model_name)\n",
    "    \n",
    "sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "classification_model.compile(optimizer = sgd, loss = \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "classification_model.fit_generator(\n",
    "                        datagen.flow(x_train, y_train, batch_size=128),\n",
    "                        epochs=300,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4,\n",
    "                        shuffle = True,\n",
    "                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
