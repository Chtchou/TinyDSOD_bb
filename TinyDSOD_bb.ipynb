{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xyrion\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Model,\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Conv2D, SeparableConv2D, concatenate, MaxPooling2D, GlobalAveragePooling2D \n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our CIFAR10 dataset directly from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the shape of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = np.unique(y_test)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'airplane',\n",
       " 1: 'automobile',\n",
       " 2: 'bird',\n",
       " 3: 'cat',\n",
       " 4: 'deer',\n",
       " 5: 'dog',\n",
       " 6: 'frog',\n",
       " 7: 'horse',\n",
       " 8: 'ship',\n",
       " 9: 'truck'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "classes = dict(zip(keys,values))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'automobile'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = np.random.choice(50000)\n",
    "plt.imshow(x_train[nb], interpolation='nearest')\n",
    "plt.show()\n",
    "classes[y_train[nb][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X_train,X_test):\n",
    "        #this function normalize inputs for zero mean and unit variance\n",
    "        # it is used when training a model.\n",
    "        # Input: training set and test set\n",
    "        # Output: normalized training set and test set according to the trianing set statistics.\n",
    "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7)\n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = normalize(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot vector encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the ddb_b block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddb_b(X_input, growth, repeat=6, **kwargs):\n",
    "    X = X_input\n",
    "    for i in range(repeat):\n",
    "        X = Conv2D(growth, (1,1), strides = 1, **kwargs)(X_input)\n",
    "        X = BatchNormalization()(X)\n",
    "        X = Activation('relu')(X)\n",
    "        X = SeparableConv2D(growth, (3,3), strides=1, **kwargs)(X)\n",
    "        X = BatchNormalization()(X)\n",
    "        X = Activation('relu')(X)\n",
    "        \n",
    "        X_input = concatenate([X_input, X], axis = -1)\n",
    "    return X_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tinyDSOD_backbone(input_shape, l2):\n",
    "    regul = regularizers.l2(l2)\n",
    "    kwargs = {'padding':'same', 'kernel_regularizer':regul,'kernel_initializer':'glorot_uniform'}\n",
    "\n",
    "    ### STEM ###\n",
    "    # Convolution 1\n",
    "    inp = Input(input_shape)\n",
    "    X = Conv2D(64, (3,3), strides=2, **kwargs)(inp)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Convolution 2\n",
    "    X = Conv2D(64, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Depth-wise seperable convolution 1\n",
    "    X = SeparableConv2D(64, (3,3), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Convolution 3\n",
    "    X = Conv2D(128, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Depth-wise seperable convolution 2\n",
    "    X = SeparableConv2D(128, (3,3), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Pooling\n",
    "    X = MaxPooling2D((2,2), strides=2, padding='same')(X)\n",
    "    \n",
    "    ### Extractor ###\n",
    "    # Dense stage 0\n",
    "    X = ddb_b(X, 32, repeat=4, **kwargs)\n",
    "    \n",
    "    # Transition layer 0\n",
    "    X = Conv2D(128, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2,2), strides=2, padding='same')(X)\n",
    "    \n",
    "    # Dense stage 1\n",
    "    X = ddb_b(X, 48, repeat = 6, **kwargs)\n",
    "    \n",
    "    # Transition layer 1\n",
    "    X = Conv2D(128, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((2,2), strides=2, padding='same')(X)\n",
    "    \n",
    "    # Dense stage 2\n",
    "    X = ddb_b(X, 64, repeat=6, **kwargs)\n",
    "    \n",
    "    # Transition layer 2\n",
    "    X = Conv2D(256, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Dense stage 3\n",
    "    X = ddb_b(X, 80, repeat=6, **kwargs)\n",
    "    \n",
    "    # Transition layer 3\n",
    "    X = Conv2D(64, (1,1), strides=1, **kwargs)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    \n",
    "    ### Classification layers ###\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    X = Dense(10, activation='softmax', kernel_regularizer=regul, kernel_initializer='glorot_uniform')(X)\n",
    "    \n",
    "    ### Create Model ### \n",
    "    \n",
    "    model = Model(inputs=inp, outputs=X, name='TinyDSOD_bb')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = tinyDSOD_backbone((32,32,3), l2=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 64)   1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 64)   4160        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 16, 16, 64)   4736        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 128)  8320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 16, 16, 128)  17664       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 128)  512         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 128)    0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 32)     4128        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 32)     128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 32)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 8, 8, 32)     1344        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 32)     128         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 32)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 160)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 8, 32)     5152        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 32)     128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 32)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 8, 8, 32)     1344        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 32)     128         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 32)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 192)    0           concatenate_1[0][0]              \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 32)     6176        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 32)     128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 32)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 8, 8, 32)     1344        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 32)     128         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 32)     0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 8, 8, 224)    0           concatenate_2[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 8, 32)     7200        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 32)     128         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 32)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 8, 8, 32)     1344        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 32)     128         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 32)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8, 8, 256)    0           concatenate_3[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 128)    32896       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 128)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 128)    0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 4, 4, 48)     6192        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 4, 4, 48)     192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 4, 4, 48)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 4, 4, 48)     2784        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 4, 4, 48)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 4, 4, 176)    0           max_pooling2d_2[0][0]            \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 4, 4, 48)     8496        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4, 4, 48)     192         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 4, 4, 48)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 4, 4, 48)     2784        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 4, 4, 48)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 4, 4, 224)    0           concatenate_5[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 4, 4, 48)     10800       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 4, 4, 48)     192         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 4, 4, 48)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 4, 4, 48)     2784        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 4, 4, 48)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 4, 4, 272)    0           concatenate_6[0][0]              \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 4, 4, 48)     13104       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 4, 4, 48)     192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 4, 4, 48)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 4, 4, 48)     2784        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 48)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 4, 4, 320)    0           concatenate_7[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 4, 4, 48)     15408       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 4, 4, 48)     192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 48)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 4, 4, 48)     2784        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 48)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 4, 4, 368)    0           concatenate_8[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 4, 4, 48)     17712       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 48)     192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 48)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 4, 4, 48)     2784        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 48)     192         separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 48)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 4, 4, 416)    0           concatenate_9[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 4, 4, 128)    53376       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 128)    512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 128)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 2, 2, 128)    0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 2, 2, 64)     8256        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 2, 2, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 2, 2, 64)     0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 2, 2, 64)     4736        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 2, 2, 64)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 2, 2, 192)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 2, 2, 64)     12352       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 2, 2, 64)     256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 2, 2, 64)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 2, 2, 64)     4736        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 2, 2, 64)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 2, 2, 256)    0           concatenate_11[0][0]             \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 2, 2, 64)     16448       concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 2, 2, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 2, 2, 64)     0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 2, 2, 64)     4736        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 2, 2, 64)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 2, 2, 320)    0           concatenate_12[0][0]             \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 2, 2, 64)     20544       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 2, 2, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 2, 2, 64)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 2, 2, 64)     4736        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 2, 2, 64)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 2, 2, 384)    0           concatenate_13[0][0]             \n",
      "                                                                 activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 2, 2, 64)     24640       concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 2, 2, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 2, 2, 64)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 2, 2, 64)     4736        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 2, 2, 64)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 2, 2, 448)    0           concatenate_14[0][0]             \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 2, 2, 64)     28736       concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 2, 2, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 2, 2, 64)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 2, 2, 64)     4736        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 2, 2, 64)     256         separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 2, 2, 64)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 2, 2, 512)    0           concatenate_15[0][0]             \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 2, 2, 256)    131328      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 2, 2, 256)    1024        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 2, 2, 256)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 2, 2, 80)     20560       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 2, 2, 80)     320         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 2, 2, 80)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 2, 2, 80)     7200        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 2, 2, 80)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 2, 2, 336)    0           activation_40[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 2, 2, 80)     26960       concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 2, 2, 80)     320         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 2, 2, 80)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 2, 2, 80)     7200        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 2, 2, 80)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 2, 2, 416)    0           concatenate_17[0][0]             \n",
      "                                                                 activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 2, 2, 80)     33360       concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 2, 2, 80)     320         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 2, 2, 80)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 2, 2, 80)     7200        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 2, 2, 80)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 2, 2, 496)    0           concatenate_18[0][0]             \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 2, 2, 80)     39760       concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 2, 2, 80)     320         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 2, 2, 80)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 2, 2, 80)     7200        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 2, 2, 80)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 2, 2, 576)    0           concatenate_19[0][0]             \n",
      "                                                                 activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 2, 2, 80)     46160       concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 2, 2, 80)     320         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 2, 2, 80)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 2, 2, 80)     7200        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 2, 2, 80)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 2, 2, 656)    0           concatenate_20[0][0]             \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 2, 2, 80)     52560       concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 2, 2, 80)     320         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 2, 2, 80)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 2, 2, 80)     7200        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 2, 2, 80)     320         separable_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 2, 2, 80)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 2, 2, 736)    0           concatenate_21[0][0]             \n",
      "                                                                 activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 2, 2, 64)     47168       concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 2, 2, 64)     256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 2, 2, 64)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 834,826\n",
      "Trainable params: 827,658\n",
      "Non-trainable params: 7,168\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " datagen = ImageDataGenerator(\n",
    "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL.h5\n",
      "Epoch 1/300\n",
      "391/391 [==============================] - 64s 164ms/step - loss: 2.8208 - acc: 0.4359 - val_loss: 2.5351 - val_acc: 0.4325\n",
      "Epoch 2/300\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 1.8887 - acc: 0.5795 - val_loss: 2.0169 - val_acc: 0.5279\n",
      "Epoch 3/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 1.5068 - acc: 0.6316 - val_loss: 1.5165 - val_acc: 0.6109\n",
      "Epoch 4/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 1.3179 - acc: 0.6588 - val_loss: 1.4858 - val_acc: 0.6007\n",
      "Epoch 5/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 1.2052 - acc: 0.6905 - val_loss: 1.2353 - val_acc: 0.6851\n",
      "Epoch 6/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 1.1382 - acc: 0.7108 - val_loss: 1.2457 - val_acc: 0.6818\n",
      "Epoch 7/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 1.0888 - acc: 0.7306 - val_loss: 2.4875 - val_acc: 0.4352\n",
      "Epoch 8/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 1.0759 - acc: 0.7369 - val_loss: 1.5260 - val_acc: 0.6104\n",
      "Epoch 9/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 1.0442 - acc: 0.7496 - val_loss: 1.6300 - val_acc: 0.6053\n",
      "Epoch 10/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 1.0325 - acc: 0.7565 - val_loss: 1.1336 - val_acc: 0.7256\n",
      "Epoch 11/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 1.0159 - acc: 0.7634 - val_loss: 1.1247 - val_acc: 0.7296\n",
      "Epoch 12/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 1.0049 - acc: 0.7671 - val_loss: 1.3635 - val_acc: 0.6789\n",
      "Epoch 13/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.9902 - acc: 0.7746 - val_loss: 1.2136 - val_acc: 0.7062\n",
      "Epoch 14/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.9837 - acc: 0.7764 - val_loss: 1.3510 - val_acc: 0.6694\n",
      "Epoch 15/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.9777 - acc: 0.7820 - val_loss: 1.2806 - val_acc: 0.6949\n",
      "Epoch 16/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.9698 - acc: 0.7836 - val_loss: 1.1573 - val_acc: 0.7159\n",
      "Epoch 17/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.9577 - acc: 0.7876 - val_loss: 1.1567 - val_acc: 0.7204\n",
      "Epoch 18/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.9602 - acc: 0.7889 - val_loss: 1.5683 - val_acc: 0.6235\n",
      "Epoch 19/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.9472 - acc: 0.7926 - val_loss: 1.4057 - val_acc: 0.6605\n",
      "Epoch 20/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.9374 - acc: 0.7982 - val_loss: 1.2774 - val_acc: 0.6948\n",
      "Epoch 21/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.8153 - acc: 0.8286 - val_loss: 1.0542 - val_acc: 0.7481\n",
      "Epoch 22/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7616 - acc: 0.8358 - val_loss: 0.8943 - val_acc: 0.7949\n",
      "Epoch 23/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7476 - acc: 0.8356 - val_loss: 1.1108 - val_acc: 0.7367\n",
      "Epoch 24/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7449 - acc: 0.8353 - val_loss: 0.8811 - val_acc: 0.7910\n",
      "Epoch 25/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7430 - acc: 0.8381 - val_loss: 1.0152 - val_acc: 0.7607\n",
      "Epoch 26/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.7372 - acc: 0.8377 - val_loss: 0.8921 - val_acc: 0.7862\n",
      "Epoch 27/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.7405 - acc: 0.8383 - val_loss: 0.8529 - val_acc: 0.8034\n",
      "Epoch 28/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.7387 - acc: 0.8380 - val_loss: 0.9070 - val_acc: 0.7894\n",
      "Epoch 29/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7345 - acc: 0.8432 - val_loss: 1.0234 - val_acc: 0.7621\n",
      "Epoch 30/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7365 - acc: 0.8405 - val_loss: 0.9769 - val_acc: 0.7744\n",
      "Epoch 31/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7293 - acc: 0.8445 - val_loss: 0.8985 - val_acc: 0.7853\n",
      "Epoch 32/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7305 - acc: 0.8424 - val_loss: 1.1065 - val_acc: 0.7492\n",
      "Epoch 33/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7256 - acc: 0.8474 - val_loss: 0.9064 - val_acc: 0.7867\n",
      "Epoch 34/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.7256 - acc: 0.8466 - val_loss: 0.9262 - val_acc: 0.7882\n",
      "Epoch 35/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7231 - acc: 0.8473 - val_loss: 0.9332 - val_acc: 0.7837\n",
      "Epoch 36/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.7248 - acc: 0.8483 - val_loss: 0.7638 - val_acc: 0.8414\n",
      "Epoch 37/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.7192 - acc: 0.8496 - val_loss: 0.8948 - val_acc: 0.7984\n",
      "Epoch 38/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7187 - acc: 0.8519 - val_loss: 1.0456 - val_acc: 0.7590\n",
      "Epoch 39/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.7226 - acc: 0.8485 - val_loss: 1.1111 - val_acc: 0.7522\n",
      "Epoch 40/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.7207 - acc: 0.8504 - val_loss: 0.8260 - val_acc: 0.8173\n",
      "Epoch 41/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.6286 - acc: 0.8777 - val_loss: 0.6780 - val_acc: 0.8623\n",
      "Epoch 42/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5874 - acc: 0.8860 - val_loss: 0.6540 - val_acc: 0.8655\n",
      "Epoch 43/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5674 - acc: 0.8868 - val_loss: 0.8943 - val_acc: 0.7979\n",
      "Epoch 44/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5733 - acc: 0.8812 - val_loss: 0.7186 - val_acc: 0.8358\n",
      "Epoch 45/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5628 - acc: 0.8852 - val_loss: 0.7067 - val_acc: 0.8419\n",
      "Epoch 46/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5591 - acc: 0.8852 - val_loss: 0.7094 - val_acc: 0.8428\n",
      "Epoch 47/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5596 - acc: 0.8838 - val_loss: 0.7506 - val_acc: 0.8263\n",
      "Epoch 48/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.5604 - acc: 0.8838 - val_loss: 0.7779 - val_acc: 0.8248\n",
      "Epoch 49/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.5544 - acc: 0.8872 - val_loss: 0.7352 - val_acc: 0.8351\n",
      "Epoch 50/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5530 - acc: 0.8864 - val_loss: 0.8677 - val_acc: 0.8016\n",
      "Epoch 51/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5514 - acc: 0.8872 - val_loss: 0.7058 - val_acc: 0.8429\n",
      "Epoch 52/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5532 - acc: 0.8874 - val_loss: 0.6782 - val_acc: 0.8513\n",
      "Epoch 53/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5544 - acc: 0.8876 - val_loss: 0.6819 - val_acc: 0.8506\n",
      "Epoch 54/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.5506 - acc: 0.8893 - val_loss: 0.7106 - val_acc: 0.8410\n",
      "Epoch 55/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.5540 - acc: 0.8878 - val_loss: 0.7224 - val_acc: 0.8374\n",
      "Epoch 56/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5473 - acc: 0.8913 - val_loss: 0.7437 - val_acc: 0.8344\n",
      "Epoch 57/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5531 - acc: 0.8887 - val_loss: 0.7101 - val_acc: 0.8454\n",
      "Epoch 58/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5474 - acc: 0.8917 - val_loss: 0.7939 - val_acc: 0.8230\n",
      "Epoch 59/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.5525 - acc: 0.8907 - val_loss: 0.6877 - val_acc: 0.8567\n",
      "Epoch 60/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.5465 - acc: 0.8928 - val_loss: 0.6598 - val_acc: 0.8611\n",
      "Epoch 61/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4835 - acc: 0.9125 - val_loss: 0.5928 - val_acc: 0.8801\n",
      "Epoch 62/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4512 - acc: 0.9206 - val_loss: 0.6101 - val_acc: 0.8720\n",
      "Epoch 63/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.4430 - acc: 0.9202 - val_loss: 0.6124 - val_acc: 0.8725\n",
      "Epoch 64/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4340 - acc: 0.9212 - val_loss: 0.6156 - val_acc: 0.8706\n",
      "Epoch 65/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4287 - acc: 0.9225 - val_loss: 0.6530 - val_acc: 0.8618\n",
      "Epoch 66/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4213 - acc: 0.9228 - val_loss: 0.6451 - val_acc: 0.8582\n",
      "Epoch 67/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4230 - acc: 0.9212 - val_loss: 0.6045 - val_acc: 0.8685\n",
      "Epoch 68/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4201 - acc: 0.9223 - val_loss: 0.7016 - val_acc: 0.8432\n",
      "Epoch 69/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4204 - acc: 0.9200 - val_loss: 0.6220 - val_acc: 0.8656\n",
      "Epoch 70/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4201 - acc: 0.9200 - val_loss: 0.6308 - val_acc: 0.8631\n",
      "Epoch 71/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4169 - acc: 0.9216 - val_loss: 0.6134 - val_acc: 0.8688\n",
      "Epoch 72/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4146 - acc: 0.9212 - val_loss: 0.6568 - val_acc: 0.8539\n",
      "Epoch 73/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4126 - acc: 0.9225 - val_loss: 0.5789 - val_acc: 0.8745\n",
      "Epoch 74/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.4142 - acc: 0.9217 - val_loss: 0.5797 - val_acc: 0.8763\n",
      "Epoch 75/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4125 - acc: 0.9232 - val_loss: 0.5766 - val_acc: 0.8750\n",
      "Epoch 76/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4140 - acc: 0.9234 - val_loss: 0.5664 - val_acc: 0.8823\n",
      "Epoch 77/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.4114 - acc: 0.9223 - val_loss: 0.6228 - val_acc: 0.8613\n",
      "Epoch 78/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4110 - acc: 0.9229 - val_loss: 0.6290 - val_acc: 0.8664\n",
      "Epoch 79/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.4137 - acc: 0.9224 - val_loss: 0.6398 - val_acc: 0.8629\n",
      "Epoch 80/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4097 - acc: 0.9247 - val_loss: 0.5824 - val_acc: 0.8776\n",
      "Epoch 81/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.3586 - acc: 0.9427 - val_loss: 0.5213 - val_acc: 0.8957\n",
      "Epoch 82/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.3374 - acc: 0.9480 - val_loss: 0.5988 - val_acc: 0.8759\n",
      "Epoch 83/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3267 - acc: 0.9501 - val_loss: 0.5534 - val_acc: 0.8886\n",
      "Epoch 84/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.3224 - acc: 0.9506 - val_loss: 0.5751 - val_acc: 0.8814\n",
      "Epoch 85/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.3202 - acc: 0.9508 - val_loss: 0.5548 - val_acc: 0.8846\n",
      "Epoch 86/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.3130 - acc: 0.9521 - val_loss: 0.5415 - val_acc: 0.8913\n",
      "Epoch 87/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3162 - acc: 0.9497 - val_loss: 0.5475 - val_acc: 0.8841\n",
      "Epoch 88/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.3086 - acc: 0.9523 - val_loss: 0.5894 - val_acc: 0.8787\n",
      "Epoch 89/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.3077 - acc: 0.9514 - val_loss: 0.5440 - val_acc: 0.8854\n",
      "Epoch 90/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3075 - acc: 0.9500 - val_loss: 0.5614 - val_acc: 0.8817\n",
      "Epoch 91/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.3052 - acc: 0.9508 - val_loss: 0.5779 - val_acc: 0.8777\n",
      "Epoch 92/300\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3094 - acc: 0.9500 - val_loss: 0.5624 - val_acc: 0.8825\n",
      "Epoch 93/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2979 - acc: 0.9537 - val_loss: 0.5533 - val_acc: 0.8844\n",
      "Epoch 94/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.3010 - acc: 0.9511 - val_loss: 0.5615 - val_acc: 0.8835\n",
      "Epoch 95/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.3018 - acc: 0.9507 - val_loss: 0.5774 - val_acc: 0.8772\n",
      "Epoch 96/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2999 - acc: 0.9507 - val_loss: 0.6177 - val_acc: 0.8707\n",
      "Epoch 97/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2992 - acc: 0.9513 - val_loss: 0.5599 - val_acc: 0.8832\n",
      "Epoch 98/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2989 - acc: 0.9522 - val_loss: 0.6033 - val_acc: 0.8702\n",
      "Epoch 99/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2957 - acc: 0.9528 - val_loss: 0.5443 - val_acc: 0.8877\n",
      "Epoch 100/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2982 - acc: 0.9510 - val_loss: 0.5912 - val_acc: 0.8743\n",
      "Epoch 101/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2635 - acc: 0.9637 - val_loss: 0.5540 - val_acc: 0.8829\n",
      "Epoch 102/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2493 - acc: 0.9686 - val_loss: 0.5449 - val_acc: 0.8892\n",
      "Epoch 103/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2420 - acc: 0.9700 - val_loss: 0.5406 - val_acc: 0.8887\n",
      "Epoch 104/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2368 - acc: 0.9712 - val_loss: 0.5498 - val_acc: 0.8882\n",
      "Epoch 105/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2334 - acc: 0.9715 - val_loss: 0.5586 - val_acc: 0.8882\n",
      "Epoch 106/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2324 - acc: 0.9717 - val_loss: 0.5460 - val_acc: 0.8922\n",
      "Epoch 107/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2284 - acc: 0.9725 - val_loss: 0.5648 - val_acc: 0.8862\n",
      "Epoch 108/300\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2281 - acc: 0.9720 - val_loss: 0.5442 - val_acc: 0.8887\n",
      "Epoch 109/300\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.2263 - acc: 0.9722 - val_loss: 0.5530 - val_acc: 0.8862\n",
      "Epoch 110/300\n",
      "391/391 [==============================] - 37s 96ms/step - loss: 0.2262 - acc: 0.9720 - val_loss: 0.5661 - val_acc: 0.8856\n",
      "Epoch 111/300\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.2203 - acc: 0.9734 - val_loss: 0.5346 - val_acc: 0.8937\n",
      "Epoch 112/300\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.2191 - acc: 0.9734 - val_loss: 0.5220 - val_acc: 0.8941\n",
      "Epoch 113/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.2205 - acc: 0.9723 - val_loss: 0.5513 - val_acc: 0.8901\n",
      "Epoch 114/300\n",
      "391/391 [==============================] - 46s 119ms/step - loss: 0.2212 - acc: 0.9712 - val_loss: 0.5563 - val_acc: 0.8867\n",
      "Epoch 115/300\n",
      "391/391 [==============================] - 43s 111ms/step - loss: 0.2180 - acc: 0.9726 - val_loss: 0.5388 - val_acc: 0.8883\n",
      "Epoch 116/300\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.2180 - acc: 0.9710 - val_loss: 0.5520 - val_acc: 0.8867\n",
      "Epoch 117/300\n",
      "391/391 [==============================] - 56s 143ms/step - loss: 0.2156 - acc: 0.9719 - val_loss: 0.5753 - val_acc: 0.8835\n",
      "Epoch 118/300\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.2156 - acc: 0.9722 - val_loss: 0.5705 - val_acc: 0.8811\n",
      "Epoch 119/300\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.2117 - acc: 0.9733 - val_loss: 0.5720 - val_acc: 0.8863\n",
      "Epoch 120/300\n",
      "391/391 [==============================] - 47s 121ms/step - loss: 0.2106 - acc: 0.9730 - val_loss: 0.5667 - val_acc: 0.8861\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 46s 117ms/step - loss: 0.1972 - acc: 0.9772 - val_loss: 0.5164 - val_acc: 0.8980\n",
      "Epoch 122/300\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1870 - acc: 0.9815 - val_loss: 0.5216 - val_acc: 0.8932\n",
      "Epoch 123/300\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1822 - acc: 0.9830 - val_loss: 0.5299 - val_acc: 0.8940\n",
      "Epoch 124/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1806 - acc: 0.9831 - val_loss: 0.5107 - val_acc: 0.8990\n",
      "Epoch 125/300\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1745 - acc: 0.9848 - val_loss: 0.5135 - val_acc: 0.8981\n",
      "Epoch 126/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1750 - acc: 0.9842 - val_loss: 0.5229 - val_acc: 0.8965\n",
      "Epoch 127/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1716 - acc: 0.9851 - val_loss: 0.5377 - val_acc: 0.8921\n",
      "Epoch 128/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1711 - acc: 0.9847 - val_loss: 0.5381 - val_acc: 0.8911\n",
      "Epoch 129/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1698 - acc: 0.9849 - val_loss: 0.5210 - val_acc: 0.8986\n",
      "Epoch 130/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1684 - acc: 0.9850 - val_loss: 0.5543 - val_acc: 0.8904\n",
      "Epoch 131/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1679 - acc: 0.9853 - val_loss: 0.5234 - val_acc: 0.8959\n",
      "Epoch 132/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1664 - acc: 0.9855 - val_loss: 0.5104 - val_acc: 0.9018\n",
      "Epoch 133/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1641 - acc: 0.9860 - val_loss: 0.5508 - val_acc: 0.8918\n",
      "Epoch 134/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1666 - acc: 0.9844 - val_loss: 0.5289 - val_acc: 0.8948\n",
      "Epoch 135/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1622 - acc: 0.9855 - val_loss: 0.5176 - val_acc: 0.8979\n",
      "Epoch 136/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1618 - acc: 0.9858 - val_loss: 0.5243 - val_acc: 0.9004\n",
      "Epoch 137/300\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.1617 - acc: 0.9851 - val_loss: 0.5344 - val_acc: 0.8921\n",
      "Epoch 138/300\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1592 - acc: 0.9864 - val_loss: 0.5196 - val_acc: 0.8977\n",
      "Epoch 139/300\n",
      "391/391 [==============================] - 47s 119ms/step - loss: 0.1597 - acc: 0.9855 - val_loss: 0.5404 - val_acc: 0.8950\n",
      "Epoch 140/300\n",
      "391/391 [==============================] - 46s 119ms/step - loss: 0.1595 - acc: 0.9851 - val_loss: 0.5346 - val_acc: 0.8956\n",
      "Epoch 141/300\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 0.1529 - acc: 0.9877 - val_loss: 0.5218 - val_acc: 0.8978\n",
      "Epoch 142/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.1500 - acc: 0.9883 - val_loss: 0.5068 - val_acc: 0.9007\n",
      "Epoch 143/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1476 - acc: 0.9896 - val_loss: 0.5078 - val_acc: 0.9016\n",
      "Epoch 144/300\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1451 - acc: 0.9902 - val_loss: 0.5136 - val_acc: 0.8983\n",
      "Epoch 145/300\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.1432 - acc: 0.9908 - val_loss: 0.5133 - val_acc: 0.8987\n",
      "Epoch 146/300\n",
      "391/391 [==============================] - 50s 127ms/step - loss: 0.1414 - acc: 0.9910 - val_loss: 0.5245 - val_acc: 0.8976\n",
      "Epoch 147/300\n",
      "391/391 [==============================] - 47s 119ms/step - loss: 0.1421 - acc: 0.9905 - val_loss: 0.5228 - val_acc: 0.9005\n",
      "Epoch 148/300\n",
      "391/391 [==============================] - 49s 126ms/step - loss: 0.1389 - acc: 0.9913 - val_loss: 0.5291 - val_acc: 0.8999\n",
      "Epoch 149/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.1402 - acc: 0.9910 - val_loss: 0.5205 - val_acc: 0.9008\n",
      "Epoch 150/300\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.1403 - acc: 0.9901 - val_loss: 0.5149 - val_acc: 0.9008\n",
      "Epoch 151/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.1387 - acc: 0.9903 - val_loss: 0.5234 - val_acc: 0.8985\n",
      "Epoch 152/300\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.1356 - acc: 0.9918 - val_loss: 0.5235 - val_acc: 0.8986\n",
      "Epoch 153/300\n",
      "391/391 [==============================] - 48s 122ms/step - loss: 0.1373 - acc: 0.9908 - val_loss: 0.5258 - val_acc: 0.8986\n",
      "Epoch 154/300\n",
      "391/391 [==============================] - 48s 122ms/step - loss: 0.1365 - acc: 0.9912 - val_loss: 0.5304 - val_acc: 0.8994\n",
      "Epoch 155/300\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.1367 - acc: 0.9907 - val_loss: 0.5302 - val_acc: 0.8966\n",
      "Epoch 156/300\n",
      "391/391 [==============================] - 48s 122ms/step - loss: 0.1348 - acc: 0.9915 - val_loss: 0.5357 - val_acc: 0.8979\n",
      "Epoch 157/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.1322 - acc: 0.9920 - val_loss: 0.5250 - val_acc: 0.8983\n",
      "Epoch 158/300\n",
      "391/391 [==============================] - 49s 124ms/step - loss: 0.1336 - acc: 0.9916 - val_loss: 0.5254 - val_acc: 0.9013\n",
      "Epoch 159/300\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.1333 - acc: 0.9917 - val_loss: 0.5421 - val_acc: 0.8949\n",
      "Epoch 160/300\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.1331 - acc: 0.9913 - val_loss: 0.5217 - val_acc: 0.9004\n",
      "Epoch 161/300\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.1267 - acc: 0.9934 - val_loss: 0.5200 - val_acc: 0.9011\n",
      "Epoch 162/300\n",
      "391/391 [==============================] - 47s 121ms/step - loss: 0.1294 - acc: 0.9925 - val_loss: 0.5245 - val_acc: 0.8981\n",
      "Epoch 163/300\n",
      "391/391 [==============================] - 48s 122ms/step - loss: 0.1283 - acc: 0.9926 - val_loss: 0.5194 - val_acc: 0.9006\n",
      "Epoch 164/300\n",
      "391/391 [==============================] - 49s 125ms/step - loss: 0.1279 - acc: 0.9926 - val_loss: 0.5141 - val_acc: 0.9023\n",
      "Epoch 165/300\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.1257 - acc: 0.9934 - val_loss: 0.5182 - val_acc: 0.9008\n",
      "Epoch 166/300\n",
      "391/391 [==============================] - 49s 124ms/step - loss: 0.1262 - acc: 0.9933 - val_loss: 0.5201 - val_acc: 0.8991\n",
      "Epoch 167/300\n",
      "391/391 [==============================] - 50s 127ms/step - loss: 0.1254 - acc: 0.9933 - val_loss: 0.5221 - val_acc: 0.8990\n",
      "Epoch 168/300\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.1245 - acc: 0.9936 - val_loss: 0.5247 - val_acc: 0.9006\n",
      "Epoch 169/300\n",
      "391/391 [==============================] - 50s 128ms/step - loss: 0.1241 - acc: 0.9932 - val_loss: 0.5290 - val_acc: 0.8987\n",
      "Epoch 170/300\n",
      "391/391 [==============================] - 49s 125ms/step - loss: 0.1246 - acc: 0.9933 - val_loss: 0.5196 - val_acc: 0.9000\n",
      "Epoch 171/300\n",
      "391/391 [==============================] - 54s 138ms/step - loss: 0.1234 - acc: 0.9939 - val_loss: 0.5182 - val_acc: 0.9002\n",
      "Epoch 172/300\n",
      "391/391 [==============================] - 52s 132ms/step - loss: 0.1222 - acc: 0.9944 - val_loss: 0.5251 - val_acc: 0.8984\n",
      "Epoch 173/300\n",
      "391/391 [==============================] - 50s 129ms/step - loss: 0.1225 - acc: 0.9936 - val_loss: 0.5224 - val_acc: 0.9019\n",
      "Epoch 174/300\n",
      "391/391 [==============================] - 51s 131ms/step - loss: 0.1216 - acc: 0.9942 - val_loss: 0.5336 - val_acc: 0.8991\n",
      "Epoch 175/300\n",
      "391/391 [==============================] - 52s 134ms/step - loss: 0.1205 - acc: 0.9944 - val_loss: 0.5339 - val_acc: 0.8980\n",
      "Epoch 176/300\n",
      "391/391 [==============================] - 53s 136ms/step - loss: 0.1206 - acc: 0.9944 - val_loss: 0.5273 - val_acc: 0.8992\n",
      "Epoch 177/300\n",
      "391/391 [==============================] - 54s 138ms/step - loss: 0.1205 - acc: 0.9944 - val_loss: 0.5215 - val_acc: 0.8995\n",
      "Epoch 178/300\n",
      "391/391 [==============================] - 53s 136ms/step - loss: 0.1205 - acc: 0.9941 - val_loss: 0.5254 - val_acc: 0.8993\n",
      "Epoch 179/300\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.1204 - acc: 0.9940 - val_loss: 0.5256 - val_acc: 0.9006\n",
      "Epoch 180/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1200 - acc: 0.9937 - val_loss: 0.5231 - val_acc: 0.8992\n",
      "Epoch 181/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1191 - acc: 0.9941 - val_loss: 0.5236 - val_acc: 0.8989\n",
      "Epoch 182/300\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1172 - acc: 0.9948 - val_loss: 0.5257 - val_acc: 0.8990\n",
      "Epoch 183/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1187 - acc: 0.9941 - val_loss: 0.5241 - val_acc: 0.9002\n",
      "Epoch 184/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1181 - acc: 0.9944 - val_loss: 0.5263 - val_acc: 0.8998\n",
      "Epoch 185/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1172 - acc: 0.9950 - val_loss: 0.5239 - val_acc: 0.9003\n",
      "Epoch 186/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1174 - acc: 0.9945 - val_loss: 0.5272 - val_acc: 0.9010\n",
      "Epoch 187/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1175 - acc: 0.9945 - val_loss: 0.5276 - val_acc: 0.9014\n",
      "Epoch 188/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1164 - acc: 0.9954 - val_loss: 0.5304 - val_acc: 0.8997\n",
      "Epoch 189/300\n",
      "391/391 [==============================] - 44s 114ms/step - loss: 0.1172 - acc: 0.9945 - val_loss: 0.5248 - val_acc: 0.9008\n",
      "Epoch 190/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1157 - acc: 0.9951 - val_loss: 0.5254 - val_acc: 0.9012\n",
      "Epoch 191/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1167 - acc: 0.9949 - val_loss: 0.5270 - val_acc: 0.8993\n",
      "Epoch 192/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1163 - acc: 0.9948 - val_loss: 0.5237 - val_acc: 0.9024\n",
      "Epoch 193/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1144 - acc: 0.9952 - val_loss: 0.5234 - val_acc: 0.9020\n",
      "Epoch 194/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1161 - acc: 0.9947 - val_loss: 0.5223 - val_acc: 0.9010\n",
      "Epoch 195/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1148 - acc: 0.9957 - val_loss: 0.5254 - val_acc: 0.9014\n",
      "Epoch 196/300\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1145 - acc: 0.9954 - val_loss: 0.5258 - val_acc: 0.9019\n",
      "Epoch 197/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1132 - acc: 0.9958 - val_loss: 0.5332 - val_acc: 0.8990\n",
      "Epoch 198/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1144 - acc: 0.9951 - val_loss: 0.5304 - val_acc: 0.9014\n",
      "Epoch 199/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1141 - acc: 0.9955 - val_loss: 0.5305 - val_acc: 0.9001\n",
      "Epoch 200/300\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1141 - acc: 0.9952 - val_loss: 0.5304 - val_acc: 0.8986\n",
      "Epoch 201/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1142 - acc: 0.9955 - val_loss: 0.5281 - val_acc: 0.9002\n",
      "Epoch 202/300\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1142 - acc: 0.9950 - val_loss: 0.5272 - val_acc: 0.9009\n",
      "Epoch 00202: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f101b3cbf60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "lr_drop = 20\n",
    "\n",
    "#We create a checkpoint to save the best model and add an early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=60, verbose=1, mode='min')\n",
    "def lr_scheduler(epoch):\n",
    "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "reduce_lr = LearningRateScheduler(lr_scheduler)\n",
    "callbacks_list = [early_stop, reduce_lr]\n",
    "    \n",
    "\n",
    "print(model_name)\n",
    "    \n",
    "sgd = SGD(lr=learning_rate, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "classification_model.compile(optimizer = sgd, loss = \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "classification_model.fit_generator(\n",
    "                        datagen.flow(x_train, y_train, batch_size=128),\n",
    "                        epochs=300,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4,\n",
    "                        shuffle = True,\n",
    "                        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.save_weights('cifar10_dsodtiny_bb.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach an overall accuracy of 90%. Moreover the models seems to be overfitting, we should be able to reach a slightly better accurracy with more regularization and longer training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this accuracy with other more classical models with a similar training time:\n",
    "https://github.com/kuangliu/pytorch-cifar\n",
    "\n",
    "We observe that our accuracy is closed to some popular like vgg16 (92,64%) or resnet50 (93,62%), while having a much lower number of parameters (136 millions for vgg16) and (26millions for ResNet50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
